{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, LSTM, GRU, Embedding, Concatenate, Flatten, BatchNormalization, Conv2D, MaxPooling2D, Convolution2D\n",
    "from keras.regularizers import l2,l1\n",
    "from keras import optimizers, layers, regularizers\n",
    "from keras.optimizers import SGD,Adam,RMSprop\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import math\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy.random import seed\n",
    "import nnet_survival\n",
    "#calibration\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "#Data process1\n",
    "import os\n",
    "from functools import reduce\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PHOTOMICS():\n",
    "    def __init__(self, omics, PH, clinical):\n",
    "        self.omics = omics\n",
    "        self.PH = PH\n",
    "        self.clinical = clinical\n",
    "    \n",
    "    def start_sess(self):\n",
    "        config = ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        session = InteractiveSession(config=config)\n",
    "    \n",
    "    def architecture(self):\n",
    "        #mrna_input\n",
    "        input_1 = Input(shape = (122,122,1))\n",
    "        mrna_conv_1   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(input_1)\n",
    "        mrna_bn_1     = BatchNormalization()(mrna_conv_1)\n",
    "        mrna_act_1    = Activation('relu')(mrna_bn_1)\n",
    "        mrna_pool_1   = MaxPooling2D(pool_size = (2,2))(mrna_act_1)\n",
    "\n",
    "        mrna_conv_2   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(mrna_pool_1)\n",
    "        mrna_bn_2     = BatchNormalization()(mrna_conv_2)\n",
    "        mrna_act_2    = Activation('relu')(mrna_bn_2)\n",
    "        mrna_pool_2   = MaxPooling2D(pool_size = (2,2))(mrna_act_2)\n",
    "\n",
    "        flat_1 = Flatten()(mrna_pool_2)\n",
    "\n",
    "        #meth_input\n",
    "        input_2 = Input(shape = (122,122,1))\n",
    "        meth_conv_1   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(input_2)\n",
    "        meth_bn_1     = BatchNormalization()(meth_conv_1)\n",
    "        meth_act_1    = Activation('relu')(meth_bn_1)\n",
    "        meth_pool_1   = MaxPooling2D(pool_size = (2,2))(meth_act_1)\n",
    "\n",
    "        meth_conv_2   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(meth_pool_1)\n",
    "        meth_bn_2     = BatchNormalization()(meth_conv_2)\n",
    "        meth_act_2    = Activation('relu')(meth_bn_2)\n",
    "        meth_pool_2   = MaxPooling2D(pool_size = (2,2))(meth_act_2)\n",
    "\n",
    "        flat_2 = Flatten()(meth_pool_2)\n",
    "\n",
    "        #mirna_input\n",
    "        input_3 = Input(shape = (42,42,1))\n",
    "        mirna_conv_1   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(input_3)\n",
    "        mirna_bn_1     = BatchNormalization()(mirna_conv_1)\n",
    "        mirna_act_1    = Activation('relu')(mirna_bn_1)\n",
    "        mirna_pool_1   = MaxPooling2D(pool_size = (2,2))(mirna_act_1)\n",
    "\n",
    "        mirna_conv_2   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(mirna_pool_1)\n",
    "        mirna_bn_2     = BatchNormalization()(mirna_conv_2)\n",
    "        mirna_act_2    = Activation('relu')(mirna_bn_2)\n",
    "        mirna_pool_2   = MaxPooling2D(pool_size = (2,2))(mirna_act_2)\n",
    "\n",
    "        flat_3 = Flatten()(mirna_pool_2)\n",
    "\n",
    "        #clinical_input\n",
    "        input_4 = Input(shape=(22, ), name='clinical')\n",
    "        dense = Dense(1, activation='relu', kernel_initializer='glorot_normal')(input_4)\n",
    "        #flat4 = Flatten()(dense)\n",
    "\n",
    "        if self.omics == 'mrna':\n",
    "            if self.clinical:\n",
    "                concat = Concatenate()([flat_1, dense])\n",
    "            else:\n",
    "                concat = flat1\n",
    "\n",
    "            dense_1 = Dense(256, activation = 'relu',kernel_initializer='glorot_normal')(concat)\n",
    "            dense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)     \n",
    "\n",
    "            if self.PH:\n",
    "                dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "                output  = nnet_survival.PropHazards(n_intervals)(dense_3)\n",
    "            else:\n",
    "                output = Dense(n_intervals, activation='sigmoid', kernel_initializer='he_normal')(dense_2)\n",
    "\n",
    "            if self.clinical:\n",
    "                model = Model(inputs=[input_1, input_4], outputs=[output])\n",
    "            else:\n",
    "                model = Model(inputs=[input_1], outputs=[output])\n",
    "        \n",
    "        if self.omics == 'meth':\n",
    "            if self.clinical:\n",
    "                concat = Concatenate()([flat_2, dense])\n",
    "            else:\n",
    "                concat = flat2\n",
    "\n",
    "            dense_1 = Dense(256, activation = 'relu',kernel_initializer='glorot_normal')(concat)\n",
    "            dense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n",
    "            \n",
    "            if self.PH:\n",
    "                dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "                output  = nnet_survival.PropHazards(n_intervals)(dense_3)\n",
    "            else:\n",
    "                output = Dense(n_intervals, activation='sigmoid', kernel_initializer='he_normal')(dense_2)\n",
    "            \n",
    "            if self.cinicial:\n",
    "                model = Model(inputs=[input_2, input_4], outputs=[output])\n",
    "            else:\n",
    "                model = Model(inputs=[input_2], outputs=[output])\n",
    "        \n",
    "        if self.omics == 'mirna':\n",
    "            if self.clinical:\n",
    "                concat = Concatenate()([flat_3, dense])\n",
    "            else:\n",
    "                concat = flat3\n",
    "\n",
    "            dense_1 = Dense(256, activation = 'relu',kernel_initializer='glorot_normal')(flat_3)\n",
    "            dense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n",
    "                 \n",
    "            if self.PH:\n",
    "                dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "                output  = nnet_survival.PropHazards(n_intervals)(dense_3)\n",
    "            else:\n",
    "                output = Dense(n_intervals, activation='sigmoid', kernel_initializer='he_normal')(dense_2)\n",
    "\n",
    "            if self.clinical:\n",
    "                model = Model(inputs=[input_3,input_4], outputs=[output])\n",
    "            else:\n",
    "                model = Model(inputs=[input_3], outputs=[output])\n",
    "\n",
    "        if self.omics == 'mrna_meth':\n",
    "            if self.clinical:\n",
    "                concat = Concatenate()([flat_1, flat_2, dense])\n",
    "            else:\n",
    "                concat = Concatenate()([flat_1,flat_2])\n",
    "\n",
    "            dense_1 = Dense(256, activation = 'relu',kernel_initializer='glorot_normal')(concat)\n",
    "            dense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)    \n",
    "            \n",
    "            if self.PH:\n",
    "                dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "                output  = nnet_survival.PropHazards(n_intervals)(dense_3)\n",
    "            else:\n",
    "                output = Dense(n_intervals, activation='sigmoid', kernel_initializer='he_normal')(dense_2)\n",
    "            \n",
    "            if self.clinical:\n",
    "                model = Model(inputs=[input_1,input_2,input_4], outputs=[output])\n",
    "            else:\n",
    "                model = Model(inputs=[input_1, input_2], outputs=[output])\n",
    "\n",
    "        if self.omics == 'mrna_mirna':\n",
    "            if self.clinical:\n",
    "                concat = Concatenate()([flat_1, flat_3, dense])\n",
    "            else:\n",
    "                concat = Concatenate()([flat_1,flat_3])\n",
    "\n",
    "            dense_1 = Dense(256, activation = 'relu',kernel_initializer='glorot_normal')(concat)\n",
    "            dense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n",
    "            \n",
    "            if self.PH:\n",
    "                dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "                output  = nnet_survival.PropHazards(n_intervals)(dense_3)\n",
    "            else:\n",
    "                output = Dense(n_intervals, activation='sigmoid', kernel_initializer='he_normal')(dense_2)\n",
    "\n",
    "            if self.clinical:\n",
    "                model = Model(inputs=[input_1,input_3,input_4], outputs=[output])\n",
    "            else:\n",
    "                model = Model(inputs=[input_1, input_3], outputs=[output])\n",
    "\n",
    "        if self.omics == 'mrna_meth_mirna':\n",
    "            if self.clinical:\n",
    "                concat = Concatenate()([flat_1, flat_2, flat_3, dense])\n",
    "            else:\n",
    "                concat = Concatenate()([flat_1, flat_2, flat_3])\n",
    "\n",
    "            dense_1 = Dense(256, activation = 'relu',kernel_initializer='glorot_normal')(concat)\n",
    "            #dense_1_dropout = Dropout(0.5)(dense_1)\n",
    "            dense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n",
    "            #dense_2_dropout = Dropout(0.2)(dense_2)\n",
    "            \n",
    "            if self.PH:\n",
    "                dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "                output  = nnet_survival.PropHazards(n_intervals)(dense_3)\n",
    "            else:\n",
    "                output = Dense(n_intervals, activation='sigmoid', kernel_initializer='he_normal')(dense_2)\n",
    "\n",
    "            if self.clinical:\n",
    "                model = Model(inputs=[input_1,input_2,input_3,input_4], outputs=[output])\n",
    "            else:\n",
    "                model = Model(inputs=[input_1,input_2,input_3], outputs=[output])\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Reset Keras Session\n",
    "    def reset_keras(self):\n",
    "        print(\"Restarting Keras Session...\")\n",
    "        sess = get_session()\n",
    "        clear_session()\n",
    "        sess.close()\n",
    "        sess = get_session()\n",
    "        try:\n",
    "            del model\n",
    "        except:\n",
    "            pass\n",
    "        config = ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        session = InteractiveSession(config=config)\n",
    "        print('Done!')\n",
    "\n",
    "    #Process Data\n",
    "    #Select common patients based on the number and type of omics under observation\n",
    "    #Choices of omics: [mrna, meth, mirna, mrna_meth, mrna_mirna, mrna_meth_mirna]\n",
    "\n",
    "    def input_process1(self, path_omics1, path_omics2, path_omics3):\n",
    "        print('Data processing-I...')\n",
    "        #training_list = os.listdir('data/' + path)\n",
    "        training_list1 = os.listdir('data/' + path_omics1)\n",
    "        training_list2 = os.listdir('data/' + path_omics2)\n",
    "        training_list3 = os.listdir('data/' + path_omics3)\n",
    "\n",
    "        if self.omics=='mrna':      #for only mRNA data\n",
    "            training_list=training_list1\n",
    "            shape = (len(training_list), 122, 122, 1)\n",
    "            shape_mirna = (len(training_list), 42, 42, 1)        \n",
    "\n",
    "            dataset1 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset2 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset3 = np.ndarray(shape=shape_mirna,dtype=np.float32)\n",
    "            i=0\n",
    "            for item in training_list:\n",
    "                img1 = load_img(\"data/\" + path_omics1 + '/' + item, target_size=(122,122), color_mode='grayscale')  # this is a PIL image\n",
    "                # Convert to Numpy Array\n",
    "                x1 = img_to_array(img1) \n",
    "                dataset1[i] = x1\n",
    "                i += 1\n",
    "                if i % 100 == 0:\n",
    "                    print(\"%d images to array\" % i)\n",
    "            print(\"All mrna images done!\")\n",
    "        \n",
    "        elif self.omics=='meth':      #for only Methylation data\n",
    "            training_list=training_list2\n",
    "            shape = (len(training_list), 122, 122, 1)\n",
    "            shape_mirna = (len(training_list), 42, 42, 1)        \n",
    "\n",
    "            dataset1 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset2 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset3 = np.ndarray(shape=shape_mirna,dtype=np.float32)\n",
    "            i=0\n",
    "            for item in training_list:\n",
    "                img2 = load_img(\"data/\" + path_omics2 + '/' + item, target_size=(122,122), color_mode='grayscale')  # this is a PIL image\n",
    "                # Convert to Numpy Array\n",
    "                x2 = img_to_array(img2) \n",
    "                dataset2[i] = x2\n",
    "                i += 1\n",
    "                if i % 100 == 0:\n",
    "                    print(\"%d images to array\" % i)\n",
    "            print(\"All meth images done!\")  \n",
    "\n",
    "        elif self.omics=='mirna':     #for only miRNA data\n",
    "            training_list=training_list3\n",
    "            shape = (len(training_list), 122, 122, 1)\n",
    "            shape_mirna = (len(training_list), 42, 42, 1)        \n",
    "\n",
    "            dataset1 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset2 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset3 = np.ndarray(shape=shape_mirna,dtype=np.float32)\n",
    "            i=0\n",
    "            for item in training_list:\n",
    "                img3 = load_img(\"data/\" + path_omics3 + '/' + item, target_size=(42,42), color_mode='grayscale')  # this is a PIL image\n",
    "                # Convert to Numpy Array\n",
    "                x3 = img_to_array(img3) \n",
    "                dataset3[i] = x3\n",
    "                i += 1\n",
    "                if i % 100 == 0:\n",
    "                    print(\"%d images to array\" % i)\n",
    "            print(\"All mirna images done!\")       \n",
    "\n",
    "        elif self.omics=='mrna_meth':     #for mRNA and Methylation omics\n",
    "            training_list=np.intersect1d(training_list1,training_list2)\n",
    "            print('mRNA_meth common patients:', len(training_list))\n",
    "            training_list.sort()\n",
    "            training_list = np.asarray(training_list, dtype=object)\n",
    "\n",
    "            shape = (len(training_list), 122, 122, 1)\n",
    "            shape_mirna = (len(training_list), 42, 42, 1)        \n",
    "\n",
    "            dataset1 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset2 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset3 = np.ndarray(shape=shape_mirna,dtype=np.float32)\n",
    "\n",
    "            i=0\n",
    "            for item in training_list:\n",
    "                img1 = load_img(\"data/\" + path_omics1 + '/' + item, target_size=(122,122), color_mode='grayscale')  # this is a PIL image\n",
    "                img2 = load_img(\"data/\" + path_omics2 + '/' + item, target_size=(122,122), color_mode='grayscale')  # this is a PIL image\n",
    "                # Convert to Numpy Array\n",
    "                x1 = img_to_array(img1) \n",
    "                x2 = img_to_array(img2)  \n",
    "                dataset1[i] = x1\n",
    "                dataset2[i] = x2\n",
    "                i += 1\n",
    "                if i % 100 == 0:\n",
    "                    print(\"%d images to array\" % i)\n",
    "            print(\"All mrna_meth images done!\")\n",
    "\n",
    "        elif self.omics=='mrna_mirna':        #for mRNA and miRNA omics\n",
    "            training_list=np.intersect1d(training_list1,training_list3)\n",
    "            print('mrna_mirna common patients:', len(training_list))\n",
    "            training_list.sort()\n",
    "            training_list = np.asarray(training_list, dtype=object)\n",
    "\n",
    "            shape = (len(training_list), 122, 122, 1)\n",
    "            shape_mirna = (len(training_list), 42, 42, 1)        \n",
    "\n",
    "            dataset1 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset2 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset3 = np.ndarray(shape=shape_mirna,dtype=np.float32)\n",
    "\n",
    "            i=0\n",
    "            for item in training_list:\n",
    "                img1 = load_img(\"data/\" + path_omics1 + '/' + item, target_size=(122,122), color_mode='grayscale')  # this is a PIL image\n",
    "                img3 = load_img(\"data/\" + path_omics3 + '/' + item, target_size=(42,42), color_mode='grayscale')  # this is a PIL image\n",
    "                # Convert to Numpy Array\n",
    "                x1 = img_to_array(img1) \n",
    "                x3 = img_to_array(img3)  \n",
    "                dataset1[i] = x1\n",
    "                dataset3[i] = x3\n",
    "                i += 1\n",
    "                if i % 100 == 0:\n",
    "                    print(\"%d images to array\" % i)\n",
    "            print(\"All mrna_mirna images done!\")\n",
    "\n",
    "        elif self.omics=='mrna_meth_mirna':       #for mRNA, Methylation and miRNA omics\n",
    "            training_list = reduce(np.intersect1d, (training_list1, training_list3, training_list2))\n",
    "            training_list.sort()    \n",
    "            print('mrna_meth_mirna common patients:', len(training_list))\n",
    "            training_list = np.asarray(training_list, dtype=object)\n",
    "            #Reference: https://www.kaggle.com/lgmoneda/data-augmentation-regression\n",
    "\n",
    "            shape = (len(training_list), 122, 122, 1)\n",
    "            shape_mirna = (len(training_list), 42, 42, 1)        \n",
    "\n",
    "            dataset1 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset2 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "            dataset3 = np.ndarray(shape=shape_mirna,dtype=np.float32)\n",
    "\n",
    "            i = 0\n",
    "            for item in training_list:\n",
    "                img1 = load_img(\"data/\" + path_omics1 + '/' + item, target_size=(122,122), color_mode='grayscale')  # this is a PIL image\n",
    "                img2 = load_img(\"data/\" + path_omics2 + '/' + item, target_size=(122,122), color_mode='grayscale')  # this is a PIL image\n",
    "                img3 = load_img(\"data/\" + path_omics3 + '/' + item, target_size=(42,42), color_mode='grayscale')  # this is a PIL image\n",
    "                # Convert to Numpy Array\n",
    "                x1 = img_to_array(img1) \n",
    "                x2 = img_to_array(img2)  \n",
    "                x3 = img_to_array(img3)\n",
    "                #x = x.reshape((3, 120, 160))\n",
    "                # Normalize\n",
    "                #x = (x - 128.0) / 128.0\n",
    "                dataset1[i] = x1\n",
    "                dataset2[i] = x2\n",
    "                dataset3[i] = x3\n",
    "                i += 1\n",
    "                if i % 100 == 0:\n",
    "                    print(\"%d images to array\" % i)\n",
    "            print(\"All mrna_meth_mirna images done!!\")\n",
    "\n",
    "        return dataset1, dataset2, dataset3, training_list\n",
    "\n",
    "    #Data process2\n",
    "    def input_process2(self):\n",
    "        print(\"Data processing-II...\")\n",
    "        sample, t, f, age = [], [], [], []\n",
    "\n",
    "        for list in tqdm(training_list):\n",
    "            for i in range(len(clinical)):\n",
    "                if clinical.iloc[i]['sample'] + '.png' == str(list):\n",
    "                    p_id = clinical.iloc[i]['sample']\n",
    "                    time = clinical.iloc[i]['os_time']\n",
    "                    status = clinical.iloc[i]['vital_status']\n",
    "                    a = clinical.iloc[i]['age']\n",
    "\n",
    "                    sample.append(p_id)\n",
    "                    t.append(time)\n",
    "                    f.append(status)\n",
    "                    age.append(a)\n",
    "                    continue\n",
    "                else:\n",
    "                    pass\n",
    "        t  = np.asarray(t)\n",
    "        f  = np.asarray(f)\n",
    "        sample  = np.asarray(sample)\n",
    "        age = np.asarray(age)\n",
    "\n",
    "        br=np.arange(0.,365.*10,365./4)\n",
    "        nl=len(br)-1\n",
    "        y_t = nnet_survival.make_surv_array(t,f,br)\n",
    "        ind = range(len(f))\n",
    "        print('Done!')\n",
    "\n",
    "        if self.omics=='mrna':\n",
    "            rand_range=[1,2]\n",
    "        if self.omics=='meth':\n",
    "            rand_range=[3,4]\n",
    "        if self.omics=='mirna':\n",
    "            rand_range=[4,5]\n",
    "        if self.omics=='mrna_meth':\n",
    "            rand_range=[6,7]\n",
    "        if self.omics=='mrna_mirna':\n",
    "            rand_range=[8,9]\n",
    "        if self.omics=='mrna_meth_mirna':\n",
    "            rand_range=[10,11]\n",
    "\n",
    "        return t, f, sample, age, br, nl, y_t, ind, rand_range\n",
    "\n",
    "    def train_val_results(self, model):\n",
    "        # Inference: mrna\n",
    "        if self.omics=='mrna':\n",
    "            if self.clinical:\n",
    "                pred_train = model.predict([X_train_mrna, clinical_train], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_mrna, clinical_test], verbose=0, batch_size=batch_size)\n",
    "            else:\n",
    "                pred_train = model.predict([X_train_mrna], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_mrna], verbose=0, batch_size=batch_size)\n",
    "\n",
    "        # Inference: meth\n",
    "        if self.omics=='meth':\n",
    "            if self.clinical:\n",
    "                pred_train = model.predict([X_train_meth, clinical_train], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_meth, clinical_test], verbose=0, batch_size=batch_size)\n",
    "            else:  \n",
    "                pred_train = model.predict([X_train_meth], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_meth], verbose=0, batch_size=batch_size)\n",
    "\n",
    "        # Inference: mirna\n",
    "        if self.omics=='mirna':\n",
    "            if self.clinical:\n",
    "                pred_train = model.predict([X_train_mirna, clinical_train], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_mirna, clinical_test], verbose=0, batch_size=batch_size)\n",
    "            else:  \n",
    "                pred_train = model.predict([X_train_mirna], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_mirna], verbose=0, batch_size=batch_size)\n",
    "\n",
    "        # Inference: mrna+meth\n",
    "        if self.omics=='mrna_meth':\n",
    "            if self.clinical:\n",
    "                pred_train = model.predict([X_train_mrna, X_train_meth, clinical_train], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_mrna, X_test_meth, clinical_test], verbose=0, batch_size=batch_size)\n",
    "            else:  \n",
    "                pred_train = model.predict([X_train_mrna,X_train_meth], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_mrna, X_test_meth], verbose=0, batch_size=batch_size)\n",
    "\n",
    "        # Inference: mrna+mirna\n",
    "        if self.omics=='mrna_mirna':\n",
    "            if self.clinical:\n",
    "                pred_train = model.predict([X_train_mrna, X_train_mirna, clinical_train], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_mrna, X_test_mirna, clinical_test], verbose=0, batch_size=batch_size)\n",
    "            else:  \n",
    "                pred_train = model.predict([X_train_mrna,X_train_mirna], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_mrna, X_test_mirna], verbose=0, batch_size=batch_size)\n",
    "\n",
    "        # Inference: mrna+meth+mirna\n",
    "        if self.omics=='mrna_meth_mirna':\n",
    "            if self.clinical:\n",
    "                pred_train = model.predict([X_train_mrna, X_train_meth, X_train_mirna, clinical_train], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_mrna, X_test_meth, X_test_mirna, clinical_test], verbose=0, batch_size=batch_size)\n",
    "            else:  \n",
    "                pred_train = model.predict([X_train_mrna, X_train_meth, X_train_mirna], verbose=0, batch_size=batch_size)\n",
    "                pred_val = model.predict([X_test_mrna, X_test_meth, X_test_mirna], verbose=0, batch_size=batch_size)     \n",
    "        return pred_train, pred_val\n",
    "\n",
    "    def surv_prob(self, pred, pred_val, year):\n",
    "        prob = np.cumprod(pred[:,0:np.nonzero(breaks>365*year)[0][0]], axis=1)[:,-1]\n",
    "        prob_val = np.cumprod(pred_val[:,0:np.nonzero(breaks>365*year)[0][0]], axis=1)[:,-1]\n",
    "        median = np.median(prob)\n",
    "        median_val = np.median(prob_val)\n",
    "        #print(\"Training and validation median probabilities are:\", median, median_val)\n",
    "\n",
    "        return prob, prob_val, median, median_val\n",
    "    \n",
    "    def process3(self, clinical, train_list):\n",
    "        print('\\nProcessing clinical features')\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        one_hot_T = pd.DataFrame(enc.fit_transform(clinical[['pathology_T_stage']]).toarray())\n",
    "        one_hot_N = pd.DataFrame(enc.fit_transform(clinical[['pathology_N_stage']]).toarray())\n",
    "        one_hot_M = pd.DataFrame(enc.fit_transform(clinical[['pathology_M_stage']]).toarray())\n",
    "        one_hot_G = pd.DataFrame(enc.fit_transform(clinical[['gender']]).toarray())\n",
    "\n",
    "        clinical_feat = pd.concat([one_hot_T, one_hot_N, one_hot_M, one_hot_G, clinical['age']], axis=1)\n",
    "        clinical_feat = clinical_feat.reset_index(drop=True)\n",
    "        clinical_feat = clinical_feat.set_index([clinical['sample'].values])\n",
    "\n",
    "        train_id = []\n",
    "        for patient in tqdm(training_list):\n",
    "            train_id.append(patient.split('.')[0])\n",
    "        clinical_feat = clinical_feat.loc[train_id,:]\n",
    "        print(\"Features Processed\")\n",
    "\n",
    "        return clinical_feat, train_id\n",
    "\n",
    "\n",
    "    def metrices(self, T, surv_prob, F, y, year, train_val, median):\n",
    "        brier_true = np.cumprod(y[:,0:np.nonzero(breaks>365*year)[0][0]], axis=1)[:,-1]\n",
    "        conc = concordance_index(T, surv_prob, F)\n",
    "        brier = brier_score_loss(brier_true, surv_prob)\n",
    "        \n",
    "        T1 = T[surv_prob >= median]\n",
    "        T2 = T[surv_prob < median]\n",
    "        E1 = F[surv_prob >= median]\n",
    "        E2 = F[surv_prob < median]\n",
    "        result = logrank_test(T1, T2, E1, E2)\n",
    "        p = result.p_value\n",
    "\n",
    "        plt.rc('font', family='serif')\n",
    "        plt.rc('xtick', labelsize='x-small')\n",
    "        plt.rc('ytick', labelsize='x-small')\n",
    "\n",
    "        # fig, ax = plt.subplots(ncols=1, figsize=(8,8))\n",
    "        # #plt.figure(figsize=(12,4))\n",
    "        # #plt.subplot(1,2,1)\n",
    "        # days_plot = 9*365\n",
    "\n",
    "        # kmf = KaplanMeierFitter()\n",
    "        # for i in range(2):\n",
    "        #     if i==0:\n",
    "        #         kmf.fit(T1, event_observed = E1)\n",
    "        #     elif i==1:\n",
    "        #         kmf.fit(T2, event_observed = E2)\n",
    "        #     kmf.plot()  \n",
    "        # N1='N='+ str(len(T1))\n",
    "        # N2='N='+ str(len(T2))\n",
    "\n",
    "        # ax.set_xticks(np.arange(0, days_plot, 365))\n",
    "        # ax.set_yticks(np.arange(0, 1.125, 0.125))\n",
    "        # ax.tick_params(axis='x', labelsize=12)\n",
    "        # ax.tick_params(axis='y', labelsize=12)\n",
    "        # ax.set_xlim([0, days_plot])\n",
    "        # ax.set_ylim([0,1])\n",
    "        # ax.text(50, 0.025, 'logrank p-value = ' +str('%.3g'%(p)), bbox=dict(facecolor='red', alpha=0.3), fontsize=10)\n",
    "\n",
    "        # ax.set_xlabel('Follow-up time (days)', fontsize = 14)\n",
    "        # ax.set_ylabel('Probability of survival', fontsize = 14)\n",
    "        # ax.legend(['Low Risk Individuals ' + N1 ,'High Risk Individuals ' + N2 ])\n",
    "        # ax.set_title('%s set Kaplan-Meier Curves'%(train_val), fontweight = 'bold', fontsize = 14)\n",
    "        # ax.grid()  \n",
    "        # plt.show()\n",
    "\n",
    "        print(\"%s year %s concordance index for %s:\"%(str(year), train_val, str(self.omics)), conc)\n",
    "        print(\"%s year %s brier score for %s:\"%(str(year), train_val, str(self.omics)), brier)\n",
    "        print(\"P-value:\", p)\n",
    "        return conc, brier, p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGO = 'tsne'\n",
    "OMICS = 'mrna_meth_mirna'\n",
    "PH = 'PH'\n",
    "# obj = PHOTOMICS('mrna')\n",
    "#obj2 = PHOTOMICS('meth')\n",
    "# obj3 = PHOTOMICS('mirna')\n",
    "#obj = PHOTOMICS('mrna_meth')\n",
    "obj = PHOTOMICS(OMICS, PH=True, clinical=True)\n",
    "#obj = PHOTOMICS('mrna_meth_mirna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data processing-I...\nmrna_meth_mirna common patients: 446\n100 images to array\n200 images to array\n  0%|          | 0/446 [00:00<?, ?it/s]300 images to array\n400 images to array\nAll mrna_meth_mirna images done!!\n446 | 446 | 446\nData processing-II...\n100%|██████████| 446/446 [00:24<00:00, 18.00it/s]\n100%|██████████| 446/446 [00:00<00:00, 874548.66it/s]Done!\n\nProcessing clinical features\nFeatures Processed\n\n"
    }
   ],
   "source": [
    "clinical = pd.read_csv('data/clinical_data_subsets/clinical_data.csv')\n",
    "dataset_mrna, dataset_meth, dataset_mirna, training_list = obj.input_process1(ALGO+'_training_data_mrna', ALGO+'_training_data_meth', ALGO+'_training_data_mirna')\n",
    "print(len(dataset_meth),'|', len(dataset_mirna),'|', len(dataset_mrna))\n",
    "t, f, sample, age, breaks, n_intervals, y_train_array, indices, rand_range = obj.input_process2()\n",
    "clinical_feat, train_id_clinical = obj.process3(clinical, training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(446, 446, 446, 446)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "len(dataset_mrna), len(dataset_meth), len(dataset_mirna), len(clinical_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nEpoch 32/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4471 - val_loss: 1.5643\n\nEpoch 00032: val_loss did not improve from 1.55780\nEpoch 33/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4193 - val_loss: 2.2453\n\nEpoch 00033: val_loss did not improve from 1.55780\nEpoch 34/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4103 - val_loss: 1.6007\n\nEpoch 00034: val_loss did not improve from 1.55780\nEpoch 35/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3746 - val_loss: 1.8943\n\nEpoch 00035: val_loss did not improve from 1.55780\nEpoch 36/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3727 - val_loss: 2.0054\n\nEpoch 00036: val_loss did not improve from 1.55780\nEpoch 37/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4232 - val_loss: 1.6002\n\nEpoch 00037: val_loss did not improve from 1.55780\nEpoch 38/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4056 - val_loss: 2.0569\n\nEpoch 00038: val_loss did not improve from 1.55780\nEpoch 39/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3661 - val_loss: 1.6674\n\nEpoch 00039: val_loss did not improve from 1.55780\nEpoch 40/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3483 - val_loss: 1.6319\n\nEpoch 00040: val_loss did not improve from 1.55780\nEpoch 41/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3886 - val_loss: 2.0008\n\nEpoch 00041: val_loss did not improve from 1.55780\nEpoch 42/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4295 - val_loss: 1.5749\n\nEpoch 00042: val_loss did not improve from 1.55780\nEpoch 43/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5567 - val_loss: 1.7279\n\nEpoch 00043: val_loss did not improve from 1.55780\nEpoch 44/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4355 - val_loss: 1.8309\n\nEpoch 00044: val_loss did not improve from 1.55780\nEpoch 45/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4669 - val_loss: 1.8872\n\nEpoch 00045: val_loss did not improve from 1.55780\nEpoch 46/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4495 - val_loss: 1.6676\n\nEpoch 00046: val_loss did not improve from 1.55780\nEpoch 47/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4300 - val_loss: 1.6851\n\nEpoch 00047: val_loss did not improve from 1.55780\nEpoch 48/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5304 - val_loss: 1.9550\n\nEpoch 00048: val_loss did not improve from 1.55780\nEpoch 49/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3860 - val_loss: 2.3153\n\nEpoch 00049: val_loss did not improve from 1.55780\nEpoch 50/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3779 - val_loss: 1.7954\n\nEpoch 00050: val_loss did not improve from 1.55780\nEpoch 51/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3376 - val_loss: 1.7320\n\nEpoch 00051: val_loss did not improve from 1.55780\nEpoch 52/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3744 - val_loss: 2.0713\n\nEpoch 00052: val_loss did not improve from 1.55780\nEpoch 53/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3916 - val_loss: 1.7469\n\nEpoch 00053: val_loss did not improve from 1.55780\nEpoch 54/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3952 - val_loss: 1.6597\n\nEpoch 00054: val_loss did not improve from 1.55780\nEpoch 55/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3863 - val_loss: 1.6613\n\nEpoch 00055: val_loss did not improve from 1.55780\n5 year train concordance index for mrna_meth_mirna: 0.9081150881723902\n5 year train brier score for mrna_meth_mirna: 0.1121588785264092\nP-value: 6.5715675515798e-40\n5 year test concordance index for mrna_meth_mirna: 0.6331658291457286\n5 year test brier score for mrna_meth_mirna: 0.14420096771453753\nP-value: 0.056921221697082594\n5 year train concordance index for mrna_meth_mirna: 0.8439126750332916\n5 year train brier score for mrna_meth_mirna: 0.2290814369152402\nP-value: 2.9066639234847473e-28\n5 year test concordance index for mrna_meth_mirna: 0.6507537688442211\n5 year test brier score for mrna_meth_mirna: 0.23891792851705967\nP-value: 0.012235652435792287\nRestarting Keras Session...\nDone!\nTrain on 356 samples, validate on 90 samples\nEpoch 1/400\n356/356 [==============================] - 5s 15ms/step - loss: 2.1038 - val_loss: 1.4619\n\nEpoch 00001: val_loss improved from inf to 1.46192, saving model to checkpoints/PH/tsne_mrna_meth_mirna_clinical/two_dense_weights-improvement-17.hdf5\nEpoch 2/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.6506 - val_loss: 1.4276\n\nEpoch 00002: val_loss improved from 1.46192 to 1.42761, saving model to checkpoints/PH/tsne_mrna_meth_mirna_clinical/two_dense_weights-improvement-17.hdf5\nEpoch 3/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.6147 - val_loss: 1.6041\n\nEpoch 00003: val_loss did not improve from 1.42761\nEpoch 4/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.6589 - val_loss: 1.6411\n\nEpoch 00004: val_loss did not improve from 1.42761\nEpoch 5/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.6202 - val_loss: 1.4828\n\nEpoch 00005: val_loss did not improve from 1.42761\nEpoch 6/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.6026 - val_loss: 1.8826\n\nEpoch 00006: val_loss did not improve from 1.42761\nEpoch 7/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.6202 - val_loss: 1.6908\n\nEpoch 00007: val_loss did not improve from 1.42761\nEpoch 8/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5823 - val_loss: 1.5154\n\nEpoch 00008: val_loss did not improve from 1.42761\nEpoch 9/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5682 - val_loss: 1.6241\n\nEpoch 00009: val_loss did not improve from 1.42761\nEpoch 10/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5398 - val_loss: 1.5147\n\nEpoch 00010: val_loss did not improve from 1.42761\nEpoch 11/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5400 - val_loss: 1.6356\n\nEpoch 00011: val_loss did not improve from 1.42761\nEpoch 12/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5418 - val_loss: 1.6011\n\nEpoch 00012: val_loss did not improve from 1.42761\nEpoch 13/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5117 - val_loss: 1.6448\n\nEpoch 00013: val_loss did not improve from 1.42761\nEpoch 14/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5075 - val_loss: 1.6278\n\nEpoch 00014: val_loss did not improve from 1.42761\nEpoch 15/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5470 - val_loss: 1.5051\n\nEpoch 00015: val_loss did not improve from 1.42761\nEpoch 16/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5155 - val_loss: 1.7608\n\nEpoch 00016: val_loss did not improve from 1.42761\nEpoch 17/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5073 - val_loss: 1.5286\n\nEpoch 00017: val_loss did not improve from 1.42761\nEpoch 18/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5312 - val_loss: 1.6975\n\nEpoch 00018: val_loss did not improve from 1.42761\nEpoch 19/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5521 - val_loss: 2.0453\n\nEpoch 00019: val_loss did not improve from 1.42761\nEpoch 20/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5382 - val_loss: 1.8367\n\nEpoch 00020: val_loss did not improve from 1.42761\nEpoch 21/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4889 - val_loss: 1.7274\n\nEpoch 00021: val_loss did not improve from 1.42761\nEpoch 22/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4888 - val_loss: 1.9449\n\nEpoch 00022: val_loss did not improve from 1.42761\nEpoch 23/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4660 - val_loss: 1.8360\n\nEpoch 00023: val_loss did not improve from 1.42761\nEpoch 24/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4811 - val_loss: 1.9692\n\nEpoch 00024: val_loss did not improve from 1.42761\nEpoch 25/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5265 - val_loss: 1.5478\n\nEpoch 00025: val_loss did not improve from 1.42761\nEpoch 26/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4963 - val_loss: 1.8739\n\nEpoch 00026: val_loss did not improve from 1.42761\nEpoch 27/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4870 - val_loss: 1.7026\n\nEpoch 00027: val_loss did not improve from 1.42761\n5 year train concordance index for mrna_meth_mirna: 0.822410731899783\n5 year train brier score for mrna_meth_mirna: 0.09576602725117103\nP-value: 2.847523458550158e-29\n5 year test concordance index for mrna_meth_mirna: 0.5941422594142259\n5 year test brier score for mrna_meth_mirna: 0.1271470199294666\nP-value: 0.24668277600293143\n5 year train concordance index for mrna_meth_mirna: 0.6156243835075952\n5 year train brier score for mrna_meth_mirna: 0.1909183896232217\nP-value: 0.0004707817037062223\n5 year test concordance index for mrna_meth_mirna: 0.5348675034867504\n5 year test brier score for mrna_meth_mirna: 0.1977316331136715\nP-value: 0.426807262099009\nRestarting Keras Session...\nDone!\nException ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fc64056a3c8>>\nTraceback (most recent call last):\n  File \"/home/dell15/KING/Work_ubuntu/Projects/MO-int/king_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n    self._session._session, self._handle)\ntensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\nTrain on 356 samples, validate on 90 samples\nEpoch 1/400\n356/356 [==============================] - 5s 15ms/step - loss: 2.0347 - val_loss: 1.6159\n\nEpoch 00001: val_loss improved from inf to 1.61591, saving model to checkpoints/PH/tsne_mrna_meth_mirna_clinical/two_dense_weights-improvement-18.hdf5\nEpoch 2/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5880 - val_loss: 1.5845\n\nEpoch 00002: val_loss improved from 1.61591 to 1.58454, saving model to checkpoints/PH/tsne_mrna_meth_mirna_clinical/two_dense_weights-improvement-18.hdf5\nEpoch 3/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.6497 - val_loss: 1.6078\n\nEpoch 00003: val_loss did not improve from 1.58454\nEpoch 4/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5967 - val_loss: 1.6639\n\nEpoch 00004: val_loss did not improve from 1.58454\nEpoch 5/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.6379 - val_loss: 1.8539\n\nEpoch 00005: val_loss did not improve from 1.58454\nEpoch 6/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5791 - val_loss: 1.6608\n\nEpoch 00006: val_loss did not improve from 1.58454\nEpoch 7/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5725 - val_loss: 1.6302\n\nEpoch 00007: val_loss did not improve from 1.58454\nEpoch 8/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5316 - val_loss: 1.5402\n\nEpoch 00008: val_loss improved from 1.58454 to 1.54025, saving model to checkpoints/PH/tsne_mrna_meth_mirna_clinical/two_dense_weights-improvement-18.hdf5\nEpoch 9/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5468 - val_loss: 1.5703\n\nEpoch 00009: val_loss did not improve from 1.54025\nEpoch 10/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5281 - val_loss: 1.6534\n\nEpoch 00010: val_loss did not improve from 1.54025\nEpoch 11/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5161 - val_loss: 1.5971\n\nEpoch 00011: val_loss did not improve from 1.54025\nEpoch 12/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5077 - val_loss: 1.4735\n\nEpoch 00012: val_loss improved from 1.54025 to 1.47353, saving model to checkpoints/PH/tsne_mrna_meth_mirna_clinical/two_dense_weights-improvement-18.hdf5\nEpoch 13/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5385 - val_loss: 1.5047\n\nEpoch 00013: val_loss did not improve from 1.47353\nEpoch 14/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5376 - val_loss: 1.5339\n\nEpoch 00014: val_loss did not improve from 1.47353\nEpoch 15/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5270 - val_loss: 1.5345\n\nEpoch 00015: val_loss did not improve from 1.47353\nEpoch 16/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5229 - val_loss: 1.6201\n\nEpoch 00016: val_loss did not improve from 1.47353\nEpoch 17/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5049 - val_loss: 1.7751\n\nEpoch 00017: val_loss did not improve from 1.47353\nEpoch 18/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5034 - val_loss: 1.7840\n\nEpoch 00018: val_loss did not improve from 1.47353\nEpoch 19/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5090 - val_loss: 1.5642\n\nEpoch 00019: val_loss did not improve from 1.47353\nEpoch 20/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5202 - val_loss: 1.8274\n\nEpoch 00020: val_loss did not improve from 1.47353\nEpoch 21/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4850 - val_loss: 1.7577\n\nEpoch 00021: val_loss did not improve from 1.47353\nEpoch 22/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4569 - val_loss: 1.8087\n\nEpoch 00022: val_loss did not improve from 1.47353\nEpoch 23/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4721 - val_loss: 1.7714\n\nEpoch 00023: val_loss did not improve from 1.47353\nEpoch 24/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4852 - val_loss: 1.6539\n\nEpoch 00024: val_loss did not improve from 1.47353\nEpoch 25/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4561 - val_loss: 1.6896\n\nEpoch 00025: val_loss did not improve from 1.47353\nEpoch 26/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4645 - val_loss: 1.7766\n\nEpoch 00026: val_loss did not improve from 1.47353\nEpoch 27/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4540 - val_loss: 1.8488\n\nEpoch 00027: val_loss did not improve from 1.47353\nEpoch 28/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4418 - val_loss: 1.8572\n\nEpoch 00028: val_loss did not improve from 1.47353\nEpoch 29/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4283 - val_loss: 1.6525\n\nEpoch 00029: val_loss did not improve from 1.47353\nEpoch 30/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.3929 - val_loss: 2.0037\n\nEpoch 00030: val_loss did not improve from 1.47353\nEpoch 31/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4339 - val_loss: 1.7635\n\nEpoch 00031: val_loss did not improve from 1.47353\nEpoch 32/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4228 - val_loss: 1.9562\n\nEpoch 00032: val_loss did not improve from 1.47353\nEpoch 33/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4867 - val_loss: 1.8747\n\nEpoch 00033: val_loss did not improve from 1.47353\nEpoch 34/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4934 - val_loss: 1.8521\n\nEpoch 00034: val_loss did not improve from 1.47353\nEpoch 35/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4475 - val_loss: 2.1299\n\nEpoch 00035: val_loss did not improve from 1.47353\nEpoch 36/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4406 - val_loss: 1.8815\n\nEpoch 00036: val_loss did not improve from 1.47353\nEpoch 37/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4541 - val_loss: 1.8669\n\nEpoch 00037: val_loss did not improve from 1.47353\n5 year train concordance index for mrna_meth_mirna: 0.8730455265402266\n5 year train brier score for mrna_meth_mirna: 0.07738260405854346\nP-value: 3.5748432825387466e-37\n5 year test concordance index for mrna_meth_mirna: 0.5701275045537341\n5 year test brier score for mrna_meth_mirna: 0.10958568373106817\nP-value: 0.2515454389707502\n5 year train concordance index for mrna_meth_mirna: 0.7601023433375299\n5 year train brier score for mrna_meth_mirna: 0.21886955244377565\nP-value: 5.364369166277045e-17\n5 year test concordance index for mrna_meth_mirna: 0.5695203400121432\n5 year test brier score for mrna_meth_mirna: 0.24299401082808592\nP-value: 0.0702510760004864\nRestarting Keras Session...\nDone!\nTrain on 356 samples, validate on 90 samples\nEpoch 1/400\n356/356 [==============================] - 5s 15ms/step - loss: 2.1093 - val_loss: 1.5080\n\nEpoch 00001: val_loss improved from inf to 1.50803, saving model to checkpoints/PH/tsne_mrna_meth_mirna_clinical/two_dense_weights-improvement-19.hdf5\nEpoch 2/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.6251 - val_loss: 1.5208\n\nEpoch 00002: val_loss did not improve from 1.50803\nEpoch 3/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5933 - val_loss: 1.8332\n\nEpoch 00003: val_loss did not improve from 1.50803\nEpoch 4/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5832 - val_loss: 1.7391\n\nEpoch 00004: val_loss did not improve from 1.50803\nEpoch 5/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5905 - val_loss: 1.6132\n\nEpoch 00005: val_loss did not improve from 1.50803\nEpoch 6/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5735 - val_loss: 1.5015\n\nEpoch 00006: val_loss improved from 1.50803 to 1.50146, saving model to checkpoints/PH/tsne_mrna_meth_mirna_clinical/two_dense_weights-improvement-19.hdf5\nEpoch 7/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5678 - val_loss: 1.7280\n\nEpoch 00007: val_loss did not improve from 1.50146\nEpoch 8/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5643 - val_loss: 1.7458\n\nEpoch 00008: val_loss did not improve from 1.50146\nEpoch 9/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5545 - val_loss: 1.7349\n\nEpoch 00009: val_loss did not improve from 1.50146\nEpoch 10/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5246 - val_loss: 2.4328\n\nEpoch 00010: val_loss did not improve from 1.50146\nEpoch 11/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5376 - val_loss: 1.8821\n\nEpoch 00011: val_loss did not improve from 1.50146\nEpoch 12/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5414 - val_loss: 1.7315\n\nEpoch 00012: val_loss did not improve from 1.50146\nEpoch 13/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5106 - val_loss: 1.9891\n\nEpoch 00013: val_loss did not improve from 1.50146\nEpoch 14/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5073 - val_loss: 1.9497\n\nEpoch 00014: val_loss did not improve from 1.50146\nEpoch 15/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4810 - val_loss: 1.6285\n\nEpoch 00015: val_loss did not improve from 1.50146\nEpoch 16/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5058 - val_loss: 1.7564\n\nEpoch 00016: val_loss did not improve from 1.50146\nEpoch 17/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.4807 - val_loss: 1.6706\n\nEpoch 00017: val_loss did not improve from 1.50146\nEpoch 18/400\n356/356 [==============================] - 5s 14ms/step - loss: 1.5269 - val_loss: 1.6808\n\nEpoch 00018: val_loss did not improve from 1.50146\nEpoch 19/400\n 64/356 [====>.........................] - ETA: 3s - loss: 1.3345"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d8bb9e760dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mT_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTF_ind_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTF_ind_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_mrna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_meth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_mirna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclinical_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_mrna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_meth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_mirna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclinical_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#Load saved best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KING/Work_ubuntu/Projects/MO-int/king_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/KING/Work_ubuntu/Projects/MO-int/king_env/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/KING/Work_ubuntu/Projects/MO-int/king_env/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/KING/Work_ubuntu/Projects/MO-int/king_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({'Conc': [], 'Brier': [], 'p_value': [], 'ConcVal': [], 'BrierVal': [], 'PVAlueVal': [], 'ConcBm': [], 'BrierBm': [], 'p_valueBm': [], 'ConcValBm': [], 'BrierValBm': [], 'PVAlueVal_Bm': []})\n",
    "\n",
    "for random in range(20):\n",
    "    seed(123)\n",
    "    tf.random.set_random_seed(123)\n",
    "\n",
    "    #Parameters for model\n",
    "    indices = range(len(f))\n",
    "    #random=3\n",
    "    split_ratio = 0.2\n",
    "    batch_size = 8\n",
    "    sgd  = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=25)\n",
    "    filepath='checkpoints/'+PH+'/'+ALGO+'_'+OMICS+'_clinical/two_dense_weights-improvement-' + str(random) + '.hdf5'\n",
    "    model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    #Initilize and compile model\n",
    "    obj.start_sess()\n",
    "    cox=obj.architecture()\n",
    "    #cox.summary()\n",
    "    cox.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=sgd)\n",
    "\n",
    "    #Test-train split\n",
    "    X_train_mrna, X_test_mrna, y_train, y_test, ind_train_1, ind_test_1 = train_test_split(dataset_mrna, y_train_array,indices, test_size=split_ratio, random_state=random)\n",
    "    X_train_meth, X_test_meth, y_train, y_test, ind_train_2, ind_test_2 = train_test_split(dataset_meth, y_train_array,indices, test_size=split_ratio, random_state=random)\n",
    "    X_train_mirna, X_test_mirna, y_train, y_test, ind_train_2, ind_test_2 = train_test_split(dataset_mirna, y_train_array,indices, test_size=split_ratio, random_state=random)\n",
    "    clinical_train, clinical_test, placeholder_train, placeholder_test, ind_train, ind_test = train_test_split(clinical_feat, y_train_array, indices, test_size=split_ratio, random_state=random)\n",
    "    T_train, T_test, F_train, F_test, TF_ind_train, TF_ind_test = train_test_split(t, f,indices, test_size=split_ratio, random_state=random)\n",
    "\n",
    "    history=cox.fit([X_train_mrna, X_train_meth, X_train_mirna, clinical_train], y_train, batch_size=batch_size, epochs=400, verbose=1, validation_data=([X_test_mrna, X_test_meth, X_test_mirna, clinical_test],y_test), callbacks=[early_stopping,model_checkpoint])\n",
    "\n",
    "    #Load saved best model\n",
    "    if PH==\"PH\":\n",
    "        cox_bm = load_model('checkpoints/'+PH+'/'+ALGO+'_'+OMICS+'_clinical/two_dense_weights-improvement-'+str(random)+'.hdf5', custom_objects={'PropHazards': nnet_survival.PropHazards(n_intervals), 'loss': nnet_survival.surv_likelihood(n_intervals)})\n",
    "    elif PH==\"non-PH\":\n",
    "        cox_bm = load_model('checkpoints/'+PH+'/'+ALGO+'_'+OMICS+'_clinical/two_dense_weights-improvement-'+str(random)+'.hdf5', custom_objects={'loss': nnet_survival.surv_likelihood(n_intervals)})\n",
    "\n",
    "\n",
    "    #Generate training and testing results for last saved and best model\n",
    "    y_pred, y_pred_val=obj.train_val_results(cox)\n",
    "    y_pred_bm, y_pred_val_bm=obj.train_val_results(cox_bm)\n",
    "\n",
    "    #Calculate surv prob and medians for last saved and best model\n",
    "    #Function surv_prob takes three arguments [training prediction(pred_y), validation prediction (y_pred_val), time(t) in years ]\n",
    "    one_year_survival_prob, one_year_survival_prob_val, one_yr_median, one_yr_median_val = obj.surv_prob(y_pred, y_pred_val, 1)\n",
    "    five_year_survival_prob, five_year_survival_prob_val, five_yr_median, five_yr_median_val = obj.surv_prob(y_pred, y_pred_val, 5)\n",
    "\n",
    "    one_year_survival_prob_bm, one_year_survival_prob_val_bm, one_yr_median_bm, one_yr_median_val_bm = obj.surv_prob(y_pred_bm, y_pred_val_bm, 1)\n",
    "    five_year_survival_prob_bm, five_year_survival_prob_val_bm, five_yr_median_bm, five_yr_median_val_bm = obj.surv_prob(y_pred_bm, y_pred_val_bm, 5)\n",
    "\n",
    "    #Calculate concordance index and brier scores for last saved and best model\n",
    "    five_yr_train_concordance, five_yr_train_brier, five_yr_p_value = obj.metrices(T_train, five_year_survival_prob, F_train, y_train, 5, 'train', five_yr_median)\n",
    "    five_yr_val_concordance, five_yr_val_brier, five_yr_p_value_val = obj.metrices(T_test, five_year_survival_prob_val, F_test, y_test, 5, 'test', five_yr_median_val)\n",
    "    five_yr_train_concordance_bm, five_yr_train_brier_bm, five_yr_p_value_bm = obj.metrices(T_train, five_year_survival_prob_bm, F_train, y_train, 5, 'train', five_yr_median_bm)\n",
    "    five_yr_val_concordance_bm, five_yr_val_brier_bm, five_yr_p_value_val_bm = obj.metrices(T_test, five_year_survival_prob_val_bm, F_test, y_test, 5, 'test', five_yr_median_val_bm)\n",
    "\n",
    "    df = {'Conc': five_yr_train_concordance,'Brier':five_yr_train_brier,'p_value':five_yr_p_value, 'ConcVal': five_yr_val_concordance,'BrierVal':five_yr_val_brier, 'PVAlueVal':five_yr_p_value_val, 'ConcBm': five_yr_train_concordance_bm,'BrierBm':five_yr_train_brier_bm,'p_valueBm':five_yr_p_value_bm, 'ConcValBm': five_yr_val_concordance_bm,'BrierValBm':five_yr_val_brier_bm, 'PVAlueVal_Bm':five_yr_p_value_val_bm}\n",
    "\n",
    "    results = results.append(df, ignore_index=True)\n",
    "    results.to_csv(ALGO+'_models/'+PH+'/'+OMICS+'_clinical/res_' + str(random) + '.csv')\n",
    "    obj.reset_keras()\n",
    "results.to_csv(ALGO+'_models/'+PH+'/'+OMICS+'_clinical/res_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # #One-year\n",
    "    # one_yr_train_concordance, one_yr_train_brier, one_yr_p_value = obj.metrices(T_train, one_year_survival_prob, F_train, y_train, 1, 'train', one_yr_median)\n",
    "    # one_yr_val_concordance, one_yr_val_brier, one_yr_p_value_val = obj.metrices(T_test, one_year_survival_prob_val, F_test, y_test, 1, 'test', one_yr_median_val)\n",
    "    # one_yr_train_concordance_bm, one_yr_train_brier_bm, one_yr_p_value_bm = obj.metrices(T_train, one_year_survival_prob_bm, F_train, y_train, 1, 'train', one_yr_median_bm) \n",
    "    # one_yr_val_concordance_bm, one_yr_val_brier_bm, one_yr_p_value_val_bm = obj.metrices(T_test, one_year_survival_prob_val_bm, F_test, y_test, 1, 'test', one_yr_median_val_bm)\n",
    "    #Five-year"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bit3de122227a47471280c7d82aa4556c4e",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}