{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bit3de122227a47471280c7d82aa4556c4e",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\nProcessing Data-I\n458 515 450\n446\n(446, 122, 122, 1)\n100 images to array\n200 images to array\n  0%|          | 0/446 [00:00<?, ?it/s]300 images to array\n400 images to array\nAll images to array!\nProcessing Data-II\n100%|██████████| 446/446 [00:23<00:00, 19.26it/s]\n"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, LSTM, GRU, Embedding, Concatenate, Flatten, BatchNormalization, Conv2D, MaxPooling2D, Convolution2D\n",
    "from keras.regularizers import l2,l1\n",
    "from keras import optimizers, layers, regularizers\n",
    "from keras.optimizers import SGD,Adam,RMSprop\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import math\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from keras.engine.topology import Layer\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "import nnet_survival\n",
    "#calibration\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "#Data process1\n",
    "import os\n",
    "from functools import reduce\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del cox # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    #config.gpu_options.visible_device_list = \"0\"\n",
    "    #set_session(tensorflow.Session(config=config))\n",
    "    config = ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = InteractiveSession(config=config)\n",
    "\n",
    "print(\"Processing Data-I\")\n",
    "\n",
    "results = pd.DataFrame({'ConcTrain': [], 'ConcVal': [], 'ConcBmTrain': [], 'ConcBmval': []})\n",
    "\n",
    "clinical = pd.read_csv('data/clinical_data_subsets/clinical_data.csv')\n",
    "def input(path_omics1, path_omics2, path_omics3):\n",
    "\n",
    "    #training_list = os.listdir('data/' + path)\n",
    "    training_list1 = os.listdir('data/' + path_omics1)\n",
    "    training_list2 = os.listdir('data/' + path_omics2)\n",
    "    training_list3 = os.listdir('data/' + path_omics3)\n",
    "\n",
    "    # training_list2.to_csv(t2.csv)\n",
    "    # training_list3.to_csv(t3.csv)\n",
    "    # training_list1.to_csv(t1.csv)\n",
    "    # print(training_list2)\n",
    "    # print(training_list3)\n",
    "    # print(training_list1)\n",
    "    \n",
    "    #training_list = set(training_list1) & set(training_list2) & set(training_list3)\n",
    "    training_list = reduce(np.intersect1d, (training_list1, training_list3, training_list2))\n",
    "    print(len(training_list1), len(training_list2), len(training_list3))\n",
    "    print(len(training_list))\n",
    "    #training_list = np.intersect1d(training_list1, training_list2, training_list3)\n",
    "\n",
    "    #folder  = 'data/' + path\n",
    "    training_list.sort()    \n",
    "\n",
    "    training_list = np.asarray(training_list, dtype=object)\n",
    "    shape = (len(training_list), 122, 122, 1)\n",
    "    shape_mirna = (len(training_list), 42, 42, 1)\n",
    "    print(shape)\n",
    "    #input_shape = (122, 122, 1)\n",
    "\n",
    "    dataset1 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "    dataset2 = np.ndarray(shape=shape,dtype=np.float32)\n",
    "    dataset3 = np.ndarray(shape=shape_mirna,dtype=np.float32)\n",
    "\n",
    "    #Reference: https://www.kaggle.com/lgmoneda/data-augmentation-regression\n",
    "\n",
    "    i = 0\n",
    "    for item in training_list:\n",
    "        img1 = load_img(\"data/\" + path_omics1 + '/' + item, target_size=(122,122), color_mode='grayscale')  # this is a PIL image\n",
    "        img2 = load_img(\"data/\" + path_omics2 + '/' + item, target_size=(122,122), color_mode='grayscale')  # this is a PIL image\n",
    "        img3 = load_img(\"data/\" + path_omics3 + '/' + item, target_size=(42,42), color_mode='grayscale')  # this is a PIL image\n",
    "        # Convert to Numpy Array\n",
    "        x1 = img_to_array(img1) \n",
    "        x2 = img_to_array(img2)  \n",
    "        x3 = img_to_array(img3)\n",
    "        #x = x.reshape((3, 120, 160))\n",
    "        # Normalize\n",
    "        #x = (x - 128.0) / 128.0\n",
    "        dataset1[i] = x1\n",
    "        dataset2[i] = x2\n",
    "        dataset3[i] = x3\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(\"%d images to array\" % i)\n",
    "    print(\"All images to array!\")\n",
    "\n",
    "    return dataset1, dataset2, dataset3, training_list\n",
    "\n",
    "dataset_meth, dataset_mrna, dataset_mirna, training_list = input('training_data_meth', 'training_data_mrna', 'training_data_mirna')\n",
    "\n",
    "#Data process2\n",
    "from tqdm import tqdm\n",
    "print(\"Processing Data-II\")\n",
    "sample = []\n",
    "t = []\n",
    "f = []\n",
    "age = []\n",
    "\n",
    "for list in tqdm(training_list):\n",
    "    for i in range(len(clinical)):\n",
    "        if clinical.iloc[i]['sample'] + '.png' == str(list):\n",
    "            p_id = clinical.iloc[i]['sample']\n",
    "            time = clinical.iloc[i]['os_time']\n",
    "            status = clinical.iloc[i]['vital_status']\n",
    "            a = clinical.iloc[i]['age']\n",
    "\n",
    "            sample.append(p_id)\n",
    "            t.append(time)\n",
    "            f.append(status)\n",
    "            age.append(a)\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "t  = np.asarray(t)\n",
    "f  = np.asarray(f)\n",
    "sample  = np.asarray(sample)\n",
    "age = np.asarray(age)\n",
    "#clinical.iloc[1]['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "055: val_loss did not improve from 1.34567\nEpoch 56/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3552 - val_loss: 1.4100\n\nEpoch 00056: val_loss did not improve from 1.34567\nEpoch 57/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3558 - val_loss: 1.3765\n\nEpoch 00057: val_loss did not improve from 1.34567\nEpoch 58/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3438 - val_loss: 1.4122\n\nEpoch 00058: val_loss did not improve from 1.34567\nEpoch 59/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3347 - val_loss: 1.3809\n\nEpoch 00059: val_loss did not improve from 1.34567\nEpoch 60/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3366 - val_loss: 1.3497\n\nEpoch 00060: val_loss did not improve from 1.34567\nEpoch 61/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3449 - val_loss: 1.3563\n\nEpoch 00061: val_loss did not improve from 1.34567\nEpoch 62/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3160 - val_loss: 1.4089\n\nEpoch 00062: val_loss did not improve from 1.34567\nEpoch 63/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3116 - val_loss: 1.3935\n\nEpoch 00063: val_loss did not improve from 1.34567\nEpoch 64/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3180 - val_loss: 1.3893\n\nEpoch 00064: val_loss did not improve from 1.34567\nEpoch 65/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3139 - val_loss: 1.3557\n\nEpoch 00065: val_loss did not improve from 1.34567\nEpoch 66/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.2950 - val_loss: 1.3934\n\nEpoch 00066: val_loss did not improve from 1.34567\nEpoch 67/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.2914 - val_loss: 1.3732\n\nEpoch 00067: val_loss did not improve from 1.34567\nEpoch 68/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.2881 - val_loss: 1.3590\n\nEpoch 00068: val_loss did not improve from 1.34567\nMediantrain | MedianTest: 0.8813281 0.85407364\nConcTrain: 0.9102279681762546\nConcVal: 0.6752738654147105\nConcBmTrain 0.8112377600979193\nConcBmval 0.6862284820031299\nBrier scores: 0.27254105346341084 0.25752632668188924\nModel: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 122, 122, 1)  0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, 122, 122, 1)  0                                            \n__________________________________________________________________________________________________\ninput_3 (InputLayer)            (None, 42, 42, 1)    0                                            \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 120, 120, 256 2560        input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 120, 120, 256 2560        input_2[0][0]                    \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 40, 40, 256)  2560        input_3[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 120, 120, 256 1024        conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 120, 120, 256 1024        conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 40, 40, 256)  1024        conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 120, 120, 256 0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 120, 120, 256 0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nactivation_5 (Activation)       (None, 40, 40, 256)  0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 60, 60, 256)  0           activation_1[0][0]               \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 60, 60, 256)  0           activation_3[0][0]               \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 20, 20, 256)  0           activation_5[0][0]               \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 58, 58, 256)  590080      max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 58, 58, 256)  590080      max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 18, 18, 256)  590080      max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 58, 58, 256)  1024        conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 58, 58, 256)  1024        conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 18, 18, 256)  1024        conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 58, 58, 256)  0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nactivation_4 (Activation)       (None, 58, 58, 256)  0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nactivation_6 (Activation)       (None, 18, 18, 256)  0           batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 29, 29, 256)  0           activation_2[0][0]               \n__________________________________________________________________________________________________\nmax_pooling2d_4 (MaxPooling2D)  (None, 29, 29, 256)  0           activation_4[0][0]               \n__________________________________________________________________________________________________\nmax_pooling2d_6 (MaxPooling2D)  (None, 9, 9, 256)    0           activation_6[0][0]               \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 215296)       0           max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nflatten_2 (Flatten)             (None, 215296)       0           max_pooling2d_4[0][0]            \n__________________________________________________________________________________________________\nflatten_3 (Flatten)             (None, 20736)        0           max_pooling2d_6[0][0]            \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 451328)       0           flatten_1[0][0]                  \n                                                                 flatten_2[0][0]                  \n                                                                 flatten_3[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          115540224   concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            256         dense_1[0][0]                    \n__________________________________________________________________________________________________\nprop_hazards_1 (PropHazards)    (None, 39)           39          dense_2[0][0]                    \n==================================================================================================\nTotal params: 117,324,583\nTrainable params: 117,321,511\nNon-trainable params: 3,072\n__________________________________________________________________________________________________\nNone\nFailed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\nTrain on 356 samples, validate on 90 samples\nEpoch 1/200\n356/356 [==============================] - 4s 12ms/step - loss: 2.4085 - val_loss: 2.2664\n\nEpoch 00001: val_loss improved from inf to 2.26637, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 2/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.5094 - val_loss: 1.7846\n\nEpoch 00002: val_loss improved from 2.26637 to 1.78462, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 3/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.5078 - val_loss: 1.7303\n\nEpoch 00003: val_loss improved from 1.78462 to 1.73029, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 4/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.5002 - val_loss: 1.7238\n\nEpoch 00004: val_loss improved from 1.73029 to 1.72383, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 5/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4986 - val_loss: 1.7187\n\nEpoch 00005: val_loss improved from 1.72383 to 1.71865, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 6/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4937 - val_loss: 1.7157\n\nEpoch 00006: val_loss improved from 1.71865 to 1.71572, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 7/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4921 - val_loss: 1.7425\n\nEpoch 00007: val_loss did not improve from 1.71572\nEpoch 8/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4921 - val_loss: 1.7105\n\nEpoch 00008: val_loss improved from 1.71572 to 1.71045, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 9/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4912 - val_loss: 1.7101\n\nEpoch 00009: val_loss improved from 1.71045 to 1.71005, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 10/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4860 - val_loss: 1.7112\n\nEpoch 00010: val_loss did not improve from 1.71005\nEpoch 11/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4848 - val_loss: 1.7039\n\nEpoch 00011: val_loss improved from 1.71005 to 1.70388, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 12/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4805 - val_loss: 1.7028\n\nEpoch 00012: val_loss improved from 1.70388 to 1.70281, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 13/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4737 - val_loss: 1.7009\n\nEpoch 00013: val_loss improved from 1.70281 to 1.70087, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 14/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4701 - val_loss: 1.6980\n\nEpoch 00014: val_loss improved from 1.70087 to 1.69802, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 15/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4712 - val_loss: 1.6972\n\nEpoch 00015: val_loss improved from 1.69802 to 1.69720, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 16/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4663 - val_loss: 1.6969\n\nEpoch 00016: val_loss improved from 1.69720 to 1.69694, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 17/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4631 - val_loss: 1.7011\n\nEpoch 00017: val_loss did not improve from 1.69694\nEpoch 18/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4591 - val_loss: 1.6915\n\nEpoch 00018: val_loss improved from 1.69694 to 1.69149, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 19/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4594 - val_loss: 1.6969\n\nEpoch 00019: val_loss did not improve from 1.69149\nEpoch 20/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4532 - val_loss: 1.7045\n\nEpoch 00020: val_loss did not improve from 1.69149\nEpoch 21/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4556 - val_loss: 1.6904\n\nEpoch 00021: val_loss improved from 1.69149 to 1.69041, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 22/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4430 - val_loss: 1.6863\n\nEpoch 00022: val_loss improved from 1.69041 to 1.68634, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 23/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4396 - val_loss: 1.6862\n\nEpoch 00023: val_loss improved from 1.68634 to 1.68623, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 24/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4391 - val_loss: 1.6842\n\nEpoch 00024: val_loss improved from 1.68623 to 1.68423, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 25/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4312 - val_loss: 1.7111\n\nEpoch 00025: val_loss did not improve from 1.68423\nEpoch 26/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4283 - val_loss: 1.6884\n\nEpoch 00026: val_loss did not improve from 1.68423\nEpoch 27/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4288 - val_loss: 1.6850\n\nEpoch 00027: val_loss did not improve from 1.68423\nEpoch 28/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4247 - val_loss: 1.6825\n\nEpoch 00028: val_loss improved from 1.68423 to 1.68248, saving model to checkpoints/mrna_meth_mirna/one_dense_weights-improvement-49.hdf5\nEpoch 29/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4154 - val_loss: 1.6896\n\nEpoch 00029: val_loss did not improve from 1.68248\nEpoch 30/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4220 - val_loss: 1.6832\n\nEpoch 00030: val_loss did not improve from 1.68248\nEpoch 31/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4119 - val_loss: 1.6995\n\nEpoch 00031: val_loss did not improve from 1.68248\nEpoch 32/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4160 - val_loss: 1.6919\n\nEpoch 00032: val_loss did not improve from 1.68248\nEpoch 33/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.4088 - val_loss: 1.6836\n\nEpoch 00033: val_loss did not improve from 1.68248\nEpoch 34/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3966 - val_loss: 1.6868\n\nEpoch 00034: val_loss did not improve from 1.68248\nEpoch 35/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3966 - val_loss: 1.6911\n\nEpoch 00035: val_loss did not improve from 1.68248\nEpoch 36/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3915 - val_loss: 1.6865\n\nEpoch 00036: val_loss did not improve from 1.68248\nEpoch 37/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3871 - val_loss: 1.7178\n\nEpoch 00037: val_loss did not improve from 1.68248\nEpoch 38/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3795 - val_loss: 1.6978\n\nEpoch 00038: val_loss did not improve from 1.68248\nEpoch 39/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3757 - val_loss: 1.6903\n\nEpoch 00039: val_loss did not improve from 1.68248\nEpoch 40/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3716 - val_loss: 1.7015\n\nEpoch 00040: val_loss did not improve from 1.68248\nEpoch 41/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3661 - val_loss: 1.6892\n\nEpoch 00041: val_loss did not improve from 1.68248\nEpoch 42/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3618 - val_loss: 1.6971\n\nEpoch 00042: val_loss did not improve from 1.68248\nEpoch 43/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3601 - val_loss: 1.6909\n\nEpoch 00043: val_loss did not improve from 1.68248\nEpoch 44/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3536 - val_loss: 1.7025\n\nEpoch 00044: val_loss did not improve from 1.68248\nEpoch 45/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3490 - val_loss: 1.7175\n\nEpoch 00045: val_loss did not improve from 1.68248\nEpoch 46/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3508 - val_loss: 1.6948\n\nEpoch 00046: val_loss did not improve from 1.68248\nEpoch 47/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3400 - val_loss: 1.7508\n\nEpoch 00047: val_loss did not improve from 1.68248\nEpoch 48/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3360 - val_loss: 1.7033\n\nEpoch 00048: val_loss did not improve from 1.68248\nEpoch 49/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3259 - val_loss: 1.7149\n\nEpoch 00049: val_loss did not improve from 1.68248\nEpoch 50/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3222 - val_loss: 1.7002\n\nEpoch 00050: val_loss did not improve from 1.68248\nEpoch 51/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3189 - val_loss: 1.7138\n\nEpoch 00051: val_loss did not improve from 1.68248\nEpoch 52/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3174 - val_loss: 1.7033\n\nEpoch 00052: val_loss did not improve from 1.68248\nEpoch 53/200\n356/356 [==============================] - 4s 12ms/step - loss: 1.3058 - val_loss: 1.7102\n\nEpoch 00053: val_loss did not improve from 1.68248\nMediantrain | MedianTest: 0.8559644 0.85224104\nConcTrain: 0.8447996678430558\nConcVal: 0.5782122905027933\nConcBmTrain 0.7484741540377828\nConcBmval 0.5670391061452514\nBrier scores: 0.2439735653366845 0.24850873730447834\n"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x360 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1,50):\n",
    "    #random seed\n",
    "    from numpy.random import seed\n",
    "    seed(123)\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_random_seed(123)\n",
    "\n",
    "    #GPUconfig\n",
    "    from tensorflow.compat.v1 import ConfigProto\n",
    "    from tensorflow.compat.v1 import InteractiveSession\n",
    "    config = ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = InteractiveSession(config=config)\n",
    "\n",
    "    breaks=np.arange(0.,365.*10,365./4)\n",
    "    n_intervals=len(breaks)-1\n",
    "    y_train_array = nnet_survival.make_surv_array(t,f,breaks)\n",
    "\n",
    "    # random  = random_state for test train split\n",
    "    indices = range(len(f))\n",
    "    random = i\n",
    "    split_ratio = 0.2\n",
    "    batch_size = 8\n",
    "\n",
    "    X_train_mrna, X_test_mrna, y_train, y_test, ind_train_1, ind_test_1 = train_test_split(dataset_mrna, y_train_array,indices, test_size=split_ratio, random_state=random)\n",
    "    X_train_meth, X_test_meth, y_train, y_test, ind_train_2, ind_test_2 = train_test_split(dataset_meth, y_train_array,indices, test_size=split_ratio, random_state=random)\n",
    "    X_train_mirna, X_test_mirna, y_train, y_test, ind_train_2, ind_test_2 = train_test_split(dataset_mirna, y_train_array,indices, test_size=split_ratio, random_state=random)\n",
    "    #age_train, age_test, placeholder_train, placeholder_test, ind_train, ind_test = train_test_split(age, y_train_array,indices, test_size=split_ratio, random_state=random)\n",
    "\n",
    "    #----------------------------------------- keras functional model for multi-omics data --------------------------\n",
    "    #input1\n",
    "\n",
    "    input_1 = Input(shape = (122,122,1))\n",
    "    mrna_conv_1   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(input_1)\n",
    "    mrna_bn_1     = BatchNormalization()(mrna_conv_1)\n",
    "    mrna_act_1    = Activation('relu')(mrna_bn_1)\n",
    "    mrna_pool_1   = MaxPooling2D(pool_size = (2,2))(mrna_act_1)\n",
    "\n",
    "    mrna_conv_2   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(mrna_pool_1)\n",
    "    mrna_bn_2     = BatchNormalization()(mrna_conv_2)\n",
    "    mrna_act_2    = Activation('relu')(mrna_bn_2)\n",
    "    mrna_pool_2   = MaxPooling2D(pool_size = (2,2))(mrna_act_2)\n",
    "\n",
    "    flat_1 = Flatten()(mrna_pool_2)\n",
    "\n",
    "    ################################################# convolutiona\n",
    "    #input2\n",
    "    input_2 = Input(shape = (122,122,1))\n",
    "    meth_conv_1   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(input_2)\n",
    "    meth_bn_1     = BatchNormalization()(meth_conv_1)\n",
    "    meth_act_1    = Activation('relu')(meth_bn_1)\n",
    "    meth_pool_1   = MaxPooling2D(pool_size = (2,2))(meth_act_1)\n",
    "\n",
    "    meth_conv_2   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(meth_pool_1)\n",
    "    meth_bn_2     = BatchNormalization()(meth_conv_2)\n",
    "    meth_act_2    = Activation('relu')(meth_bn_2)\n",
    "    meth_pool_2   = MaxPooling2D(pool_size = (2,2))(meth_act_2)\n",
    "\n",
    "    flat_2 = Flatten()(meth_pool_2)\n",
    "\n",
    "    #input3\n",
    "    input_3 = Input(shape = (42,42,1))\n",
    "    mirna_conv_1   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(input_3)\n",
    "    mirna_bn_1     = BatchNormalization()(mirna_conv_1)\n",
    "    mirna_act_1    = Activation('relu')(mirna_bn_1)\n",
    "    mirna_pool_1   = MaxPooling2D(pool_size = (2,2))(mirna_act_1)\n",
    "\n",
    "    mirna_conv_2   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(mirna_pool_1)\n",
    "    mirna_bn_2     = BatchNormalization()(mirna_conv_2)\n",
    "    mirna_act_2    = Activation('relu')(mirna_bn_2)\n",
    "    mirna_pool_2   = MaxPooling2D(pool_size = (2,2))(mirna_act_2)\n",
    "\n",
    "    flat_3 = Flatten()(mirna_pool_2)\n",
    "    #input_2_dense  = Dense(1, activation = 'relu',kernel_initializer='glorot_normal')(input_2)\n",
    "    #flat_2 = Flatten()(input_2_dense)\n",
    "\n",
    "    ################################################Phenotypes##################\n",
    "    #input_3 = Input(shape=(1, ))\n",
    "\n",
    "    concat = Concatenate()([flat_1,flat_2,flat_3])\n",
    "\n",
    "    dense_1 = Dense(256, activation = 'relu',kernel_initializer='glorot_normal')(concat)\n",
    "    #dense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n",
    "    #dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "\n",
    "    prop_hazards=1\n",
    "    if prop_hazards:\n",
    "        #dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "        \n",
    "        dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_1)\n",
    "        output  = nnet_survival.PropHazards(n_intervals)(dense_3)\n",
    "    else:\n",
    "        output = Dense(n_intervals, kernel_initializer='zeros', bias_initializer='zeros', activation = 'sigmoid')(dense_2)\n",
    "\n",
    "    cox = Model(inputs=[input_1,input_2, input_3], outputs=[output])\n",
    "\n",
    "    print(cox.summary())\n",
    "\n",
    "    from tensorflow import keras\n",
    "    #os.environ[\"PATH\"] += os.pathsep + '/home/dell15/KING/Work_ubuntu/Projects/MO-int/king_env/lib/python3.6/site-packages/graphviz'\n",
    "    keras.utils.plot_model(cox, 'multi_input_and_output_model.png', show_shapes=True)\n",
    "\n",
    "    #Gradient descent\n",
    "    sgd  = SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    cox.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=sgd)\n",
    "    #Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=25)\n",
    "    filepath='checkpoints/mrna_meth_mirna/one_dense_weights-improvement-' + str(i) + '.hdf5'\n",
    "    model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    # for functional models of multiomics\n",
    "    history=cox.fit([X_train_mrna, X_train_meth, X_train_mirna], y_train, batch_size=batch_size, epochs=200, verbose=1, validation_data=([X_test_mrna,X_test_meth, X_test_mirna],y_test), callbacks=[early_stopping,model_checkpoint])\n",
    "\n",
    "    #----------------------------------------------------------------------------- for functional model ----------------\n",
    "    #Training set results\n",
    "    y_pred=cox.predict([X_train_mrna,X_train_meth, X_train_mirna],verbose=0, batch_size = batch_size)\n",
    "    #validation set results\n",
    "    y_pred_valid=cox.predict([X_test_mrna,X_test_meth, X_test_mirna],verbose=0, batch_size = batch_size)\n",
    "\n",
    "    #-------------------------------------------- run for functional model------------------------------------\n",
    "    # load the saved model\n",
    "    saved_model = load_model('checkpoints/mrna_meth_mirna/one_dense_weights-improvement-' + str(i) + '.hdf5', custom_objects={'PropHazards': nnet_survival.PropHazards(n_intervals), 'loss': nnet_survival.surv_likelihood(n_intervals)})\n",
    "    #Training set results---------------------------- on best model for the process-------------\n",
    "    y_pred_bm=saved_model.predict([X_train_mrna,X_train_meth, X_train_mirna],verbose=0, batch_size = batch_size)\n",
    "    #validation set results\n",
    "    y_pred_valid_bm=saved_model.predict([X_test_mrna,X_test_meth, X_test_mirna],verbose=0, batch_size = batch_size)\n",
    "\n",
    "    T_train, T_test, F_train, F_test, TF_ind_train, TF_ind_test = train_test_split(t, f,indices, test_size=split_ratio, random_state=random)\n",
    "    one_year_survival_prob_train = np.cumprod(y_pred[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "    one_year_survival_prob_valid = np.cumprod(y_pred_valid[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "    one_yr_median_train = np.median(one_year_survival_prob_train)\n",
    "    one_yr_median_valid = np.median(one_year_survival_prob_valid)\n",
    "    print(\"Mediantrain | MedianTest:\", one_yr_median_train,one_yr_median_valid)\n",
    "\n",
    "    # Concordance-index------------------------------------------------------------------- 5yr-------------\n",
    "    fiveyr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "    ConcTrain = concordance_index(T_train,fiveyr_surv,F_train)\n",
    "    print(\"ConcTrain:\", concordance_index(T_train,fiveyr_surv,F_train))\n",
    "\n",
    "    # Concordance-index----- for validation set\n",
    "    fiveyr_surv_valid=np.cumprod(y_pred_valid[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "    ConcVal = concordance_index(T_test,fiveyr_surv_valid,F_test)\n",
    "    print('ConcVal:', concordance_index(T_test,fiveyr_surv_valid,F_test))\n",
    "\n",
    "    # Concordance-index----------------------------- 5yr-----------------------------------best model----------\n",
    "    fiveyr_surv_bm=np.cumprod(y_pred_bm[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "    ConcBmTrain = concordance_index(T_train,fiveyr_surv_bm,F_train)\n",
    "    print('ConcBmTrain',concordance_index(T_train,fiveyr_surv_bm,F_train))\n",
    "\n",
    "    # Concordance-index----- for validation set---------------------------------------------best model---------\n",
    "    fiveyr_surv_valid_bm=np.cumprod(y_pred_valid_bm[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "    ConcBmval = concordance_index(T_test,fiveyr_surv_valid_bm,F_test)\n",
    "    print('ConcBmval',concordance_index(T_test,fiveyr_surv_valid_bm,F_test))\n",
    "\n",
    "    #calculates True cumulative probabilitiy of 5yr survival\n",
    "    brier_true_train = np.cumprod(y_train[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "    brier_true_test = np.cumprod(y_test[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "    brier_true_all = np.cumprod(y_train_array[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "    # # plot impact of brier for single forecasts for 5yr duration\n",
    "    train_brier, test_brier = brier_score_loss(brier_true_train, fiveyr_surv), brier_score_loss(brier_true_test, fiveyr_surv_valid)\n",
    "    print('Brier scores:', train_brier, test_brier)\n",
    "\n",
    "    df = {'ConcTrain': ConcTrain, 'ConcVal': ConcVal, 'ConcBmTrain': ConcBmTrain, 'ConcBmval':ConcBmval, 'BrierTrain': train_brier, 'BrierTest': test_brier}\n",
    "    results = results.append(df, ignore_index=True)\n",
    "\n",
    "    results.to_csv('mrna_meth_mirna/one_dense/res_' + str(i) + '.csv')\n",
    "\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    #matplotlib.style.use('default')\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    days_plot = 365*9\n",
    "    for j in range(2):\n",
    "        if j==0:\n",
    "            kmf.fit(T_train[one_year_survival_prob_train >= one_yr_median_train ], event_observed=F_train[one_year_survival_prob_train >= one_yr_median_train])\n",
    "            kmf.plot()\n",
    "            #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "        #pred_surv=np.mean(np.cumprod(y_pred_valid[F_test==i,:], axis=1),axis=0)\n",
    "        #plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='--',c='C'+str(i))\n",
    "        else:\n",
    "            kmf.fit(T_train[one_year_survival_prob_train < one_yr_median_train ], event_observed=F_train[one_year_survival_prob_train < one_yr_median_train])\n",
    "            kmf.plot()\n",
    "            #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\n",
    "    N1 = ' N= ' + str(len(T_train[one_year_survival_prob_train >= one_yr_median_train ]))\n",
    "    N2 = ' N= ' + str(len(T_train[one_year_survival_prob_train < one_yr_median_train ]))\n",
    "\n",
    "    plt.xticks(np.arange(0, days_plot, 365))\n",
    "    plt.yticks(np.arange(0, 1.125, 0.125))\n",
    "    plt.xlim([0,days_plot])\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlabel('Follow-up time (days)')\n",
    "    plt.ylabel('Probability of survival')\n",
    "    plt.legend(['Low Risk Individuals' + N1 ,'High Risk Individuals' + N2 ])\n",
    "    #\t'1: Actual','1: Predicted')\n",
    "    # \t'2: Actual','2: Predicted',\n",
    "    # \t'3: Actual','3: Predicted',\n",
    "    # \t'4: Actual','4: Predicted'])\n",
    "    plt.title('Train set Kaplan-Meier Curves', fontweight = 'bold')\n",
    "    plt.grid()\n",
    "    #plt.show()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    days_plot = 365*9\n",
    "    for j in range(2):\n",
    "        if j==0:\n",
    "            kmf.fit(T_test[one_year_survival_prob_valid >= one_yr_median_valid ], event_observed=F_test[one_year_survival_prob_valid >= one_yr_median_valid])\n",
    "            kmf.plot()\n",
    "            #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "        #pred_surv=np.mean(np.cumprod(y_pred_valid[F_test==i,:], axis=1),axis=0)\n",
    "        #plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='--',c='C'+str(i))\n",
    "        else:\n",
    "            kmf.fit(T_test[one_year_survival_prob_valid < one_yr_median_valid ], event_observed=F_test[one_year_survival_prob_valid < one_yr_median_valid])\n",
    "            kmf.plot()\n",
    "            #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\n",
    "    N1 = ' N= ' + str(len(T_test[one_year_survival_prob_valid >= one_yr_median_valid ]))\n",
    "    N2 = ' N= ' + str(len(T_test[one_year_survival_prob_valid < one_yr_median_valid ]))\n",
    "    plt.xticks(np.arange(0, days_plot, 365))\n",
    "    plt.yticks(np.arange(0, 1.125, 0.125))\n",
    "    plt.xlim([0,days_plot])\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlabel('Follow-up time (days)')\n",
    "    plt.ylabel('Probability of survival')\n",
    "    plt.legend(['Low Risk Individuals' + N1 ,'High Risk Individuals' + N2 ])\n",
    "    #\t'1: Actual','1: Predicted')\n",
    "    # \t'2: Actual','2: Predicted',\n",
    "    # \t'3: Actual','3: Predicted',\n",
    "    # \t'4: Actual','4: Predicted'])\n",
    "    plt.title('Test set Kaplan-Meier Curves', fontweight = 'bold')\n",
    "    plt.grid()\n",
    "    #plt.show()\n",
    "    plt.savefig('mrna_meth_mirna/one_dense/plots/KP_' + str(i) + '.png', format = 'png', dpi = 1200, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    reset_keras()\n",
    "\n",
    "#results.to_csv('res_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cox.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_train, age_test, placeholder_train, placeholder_test, ind_train, ind_test = train_test_split(age, y_train_array,indices, test_size=split_ratio, random_state=random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mrna.shape, X_train_meth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold-------------------------- not used because of errors; dimension of predictors not compatible\n",
    "# # define 10-fold cross validation \n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=33)\n",
    "#train, test  = kfold.split(dataset, y_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------- keras functional model for multi-omics data --------------------------\n",
    "\n",
    "input_1 = Input(shape = (122,122,1))\n",
    "conv_1   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(input_1)\n",
    "bn_1     = BatchNormalization()(conv_1)\n",
    "act_1    = Activation('relu')(bn_1)\n",
    "pool_1   = MaxPooling2D(pool_size = (2,2))(act_1)\n",
    "\n",
    "conv_2   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(pool_1)\n",
    "bn_2     = BatchNormalization()(conv_2)\n",
    "act_2    = Activation('relu')(bn_2)\n",
    "pool_2   = MaxPooling2D(pool_size = (2,2))(act_2)\n",
    "\n",
    "flat_1 = Flatten()(pool_2)\n",
    "\n",
    "################################################# convolutional above | phenotype variables below\n",
    "\n",
    "input_2 = Input(shape=(1, ))\n",
    "#input_2_dense  = Dense(1, activation = 'relu',kernel_initializer='glorot_normal')(input_2)\n",
    "#flat_2 = Flatten()(input_2_dense)\n",
    "\n",
    "\n",
    "concat = Concatenate()([flat_1,input_2])\n",
    "\n",
    "#dense_1 = Dense(256, activation = 'relu',kernel_initializer='glorot_normal')(concat)\n",
    "dense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(concat)\n",
    "#dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "\n",
    "prop_hazards=1\n",
    "if prop_hazards:\n",
    "    dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "    output  = nnet_survival.PropHazards(n_intervals)(dense_3)\n",
    "else:\n",
    "    output = Dense(n_intervals, kernel_initializer='zeros', bias_initializer='zeros', activation = 'sigmoid')(dense_2)\n",
    "\n",
    "cox = Model(inputs=[input_1,input_2], outputs=[output])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------- keras functional model for multi-omics data --------------------------\n",
    "\n",
    "input_1 = Input(shape = (122,122,1))\n",
    "mrna_conv_1   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(input_1)\n",
    "mrna_bn_1     = BatchNormalization()(mrna_conv_1)\n",
    "mrna_act_1    = Activation('relu')(mrna_bn_1)\n",
    "mrna_pool_1   = MaxPooling2D(pool_size = (2,2))(mrna_act_1)\n",
    "\n",
    "mrna_conv_2   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(mrna_pool_1)\n",
    "mrna_bn_2     = BatchNormalization()(mrna_conv_2)\n",
    "mrna_act_2    = Activation('relu')(mrna_bn_2)\n",
    "mrna_pool_2   = MaxPooling2D(pool_size = (2,2))(mrna_act_2)\n",
    "\n",
    "flat_1 = Flatten()(mrna_pool_2)\n",
    "\n",
    "################################################# convolutional above | phenotype variables below\n",
    "\n",
    "input_2 = Input(shape = (122,122,1))\n",
    "conv_1   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(input_2)\n",
    "bn_1     = BatchNormalization()(conv_1)\n",
    "act_1    = Activation('relu')(bn_1)\n",
    "pool_1   = MaxPooling2D(pool_size = (2,2))(act_1)\n",
    "\n",
    "conv_2   = Convolution2D(256, (3, 3), kernel_initializer='glorot_normal')(pool_1)\n",
    "bn_2     = BatchNormalization()(conv_2)\n",
    "act_2    = Activation('relu')(bn_2)\n",
    "pool_2   = MaxPooling2D(pool_size = (2,2))(act_2)\n",
    "\n",
    "flat_2 = Flatten()(pool_2)\n",
    "#input_2_dense  = Dense(1, activation = 'relu',kernel_initializer='glorot_normal')(input_2)\n",
    "#flat_2 = Flatten()(input_2_dense)\n",
    "\n",
    "\n",
    "concat = Concatenate()([flat_1,flat_2])\n",
    "\n",
    "dense_1 = Dense(256, activation = 'relu',kernel_initializer='glorot_normal')(concat)\n",
    "dense_2 = Dense(128, activation = 'relu',kernel_initializer='glorot_normal')(dense_1)\n",
    "#dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "\n",
    "prop_hazards=1\n",
    "if prop_hazards:\n",
    "    dense_3 = Dense(1, use_bias=0, kernel_initializer='zeros')(dense_2)\n",
    "    output  = nnet_survival.PropHazards(n_intervals)(dense_3)\n",
    "else:\n",
    "    output = Dense(n_intervals, kernel_initializer='zeros', bias_initializer='zeros', activation = 'sigmoid')(dense_2)\n",
    "\n",
    "cox = Model(inputs=[input_1,input_2], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(cox, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #keras Sequential API example https://theailearner.com/2019/01/25/multi-input-and-multi-output-models-in-keras/\n",
    "# # feature extraction from gray scale image\n",
    "# inputs = Input(shape = (28,28,1))\n",
    " \n",
    "# conv1 = Conv2D(16, (3,3), activation = 'relu', padding = \"SAME\")(inputs)\n",
    "# pool1 = MaxPooling2D(pool_size = (2,2), strides = 2)(conv1)\n",
    "# conv2 = Conv2D(32, (3,3), activation = 'relu', padding = \"SAME\")(pool1)\n",
    "# pool2 = MaxPooling2D(pool_size = (2,2), strides = 2)(conv2)\n",
    "# flat_1 = Flatten()(pool2)\n",
    " \n",
    "# # feature extraction from RGB image\n",
    "# inputs_2 = Input(shape = (28,28,3))\n",
    " \n",
    "# conv1_2 = Conv2D(16, (3,3), activation = 'relu', padding = \"SAME\")(inputs_2)\n",
    "# pool1_2 = MaxPooling2D(pool_size = (2,2), strides = 2)(conv1_2)\n",
    "# conv2_2 = Conv2D(32, (3,3), activation = 'relu', padding = \"SAME\")(pool1_2)\n",
    "# pool2_2 = MaxPooling2D(pool_size = (2,2), strides = 2)(conv2_2)\n",
    "# flat_2 = Flatten()(pool2_2)\n",
    " \n",
    "# # concatenate both feature layers and define output layer after some dense layers\n",
    "# concat = concatenate([flat_1,flat_2])\n",
    "# dense1 = Dense(512, activation = 'relu')(concat)\n",
    "# dense2 = Dense(128, activation = 'relu')(dense1)\n",
    "# dense3 = Dense(32, activation = 'relu')(dense2)\n",
    "# output = Dense(10, activation = 'softmax')(dense3)\n",
    " \n",
    "# # create model with two inputs\n",
    "# model = Model([inputs,inputs_2], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient descent\n",
    "sgd  = SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "cox.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=sgd)\n",
    "#Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "filepath='checkpoints/weights-improvement-{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sequential models\n",
    "#history=cox.fit(X_train, y_train, batch_size=batch_size, epochs=200, verbose=1, validation_data=(X_test,y_test), callbacks=[early_stopping,model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history=cox.fit(X_train, y_train, batch_size=batch_size, epochs=300, verbose=1, validation_data=(X_test,y_test), callbacks=[early_stopping,model_checkpoint], initial_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_epoch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history=cox.fit(dataset, y_train_array, batch_size=batch_size, epochs=100, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#plt.plot(hist.history['val_loss'])\n",
    "#plt.plot(hist.history['loss'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"Negative Log Likelihood Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Training Loss\",\"Validation Loss\"])\n",
    "#\"Training Loss\",\"Validation Loss\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################################################----------------------------run for sequential-\n",
    "#Training set results\n",
    "y_pred=cox.predict_proba(X_train,verbose=0, batch_size = batch_size)\n",
    "#validation set results\n",
    "y_pred_valid=cox.predict_proba(X_test,verbose=0, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------- for functional model ----------------\n",
    "#Training set results\n",
    "y_pred=cox.predict([X_train_mrna,X_train_meth],verbose=0, batch_size = batch_size)\n",
    "#validation set results\n",
    "y_pred_valid=cox.predict([X_test_mrna,X_test_meth],verbose=0, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------- run for sequential model------------------------------------\n",
    "# load the saved model\n",
    "saved_model = load_model('checkpoints/weights-improvement-32-1.5237.hdf5', custom_objects={'PropHazards': nnet_survival.PropHazards(n_intervals), 'loss': nnet_survival.surv_likelihood(n_intervals)})\n",
    "#Training set results---------------------------- on best model for the process-------------\n",
    "y_pred_bm=saved_model.predict_proba(X_train,verbose=0, batch_size = batch_size)\n",
    "#validation set results\n",
    "y_pred_valid_bm=saved_model.predict_proba(X_test,verbose=0, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_train, T_test, F_train, F_test, TF_ind_train, TF_ind_test = train_test_split(t, f,indices, test_size=split_ratio, random_state=random)\n",
    "one_year_survival_prob_train = np.cumprod(y_pred[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "one_year_survival_prob_valid = np.cumprod(y_pred_valid[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "one_yr_median_train = np.median(one_year_survival_prob_train)\n",
    "one_yr_median_valid = np.median(one_year_survival_prob_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_yr_median_train,one_yr_median_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance-index------------------------------------------------------------------- 5yr-------------\n",
    "fiveyr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "print(concordance_index(T_train,fiveyr_surv,F_train))\n",
    "\n",
    "# Concordance-index----- for validation set\n",
    "fiveyr_surv_valid=np.cumprod(y_pred_valid[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "print(concordance_index(T_test,fiveyr_surv_valid,F_test))\n",
    "\n",
    "# Concordance-index----------------------------- 5yr-----------------------------------best model----------\n",
    "fiveyr_surv_bm=np.cumprod(y_pred_bm[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "print(concordance_index(T_train,fiveyr_surv_bm,F_train))\n",
    "\n",
    "# Concordance-index----- for validation set---------------------------------------------best model---------\n",
    "fiveyr_surv_valid_bm=np.cumprod(y_pred_valid_bm[:,0:np.nonzero(breaks>1825)[0][0]], axis=1)[:,-1]\n",
    "print(concordance_index(T_test,fiveyr_surv_valid_bm,F_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibration\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from lifelines import KaplanMeierFitter\n",
    "#plt.figure(figsize=(10,5))\n",
    "#plt.subplot(1, 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierFitter()\n",
    "#matplotlib.style.use('default')\n",
    "actual = []\n",
    "predicted = []\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "days_plot = 365*9\n",
    "for i in range(2):\n",
    "    if i==0:\n",
    "        kmf.fit(T_train[one_year_survival_prob_train >= one_yr_median_train ], event_observed=F_train[one_year_survival_prob_train >= one_yr_median_train])\n",
    "        kmf.plot()\n",
    "        #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\t#pred_surv=np.mean(np.cumprod(y_pred_valid[F_test==i,:], axis=1),axis=0)\n",
    "\t#plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='--',c='C'+str(i))\n",
    "    else:\n",
    "        kmf.fit(T_train[one_year_survival_prob_train < one_yr_median_train ], event_observed=F_train[one_year_survival_prob_train < one_yr_median_train])\n",
    "        kmf.plot()\n",
    "        #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\n",
    "N1 = ' N= ' + str(len(T_train[one_year_survival_prob_train >= one_yr_median_train ]))\n",
    "N2 = ' N= ' + str(len(T_train[one_year_survival_prob_train < one_yr_median_train ]))\n",
    "\n",
    "plt.xticks(np.arange(0, days_plot, 365))\n",
    "plt.yticks(np.arange(0, 1.125, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Probability of survival')\n",
    "plt.legend(['Low Risk Individuals' + N1 ,'High Risk Individuals' + N2 ])\n",
    "#\t'1: Actual','1: Predicted')\n",
    "# \t'2: Actual','2: Predicted',\n",
    "# \t'3: Actual','3: Predicted',\n",
    "# \t'4: Actual','4: Predicted'])\n",
    "plt.title('Train set Kaplan-Meier Curves', fontweight = 'bold')\n",
    "plt.grid()\n",
    "#plt.show()\n",
    "plt.subplot(1, 2, 2)\n",
    "days_plot = 365*9\n",
    "for i in range(2):\n",
    "    if i==0:\n",
    "        kmf.fit(T_test[one_year_survival_prob_valid >= one_yr_median_valid ], event_observed=F_test[one_year_survival_prob_valid >= one_yr_median_valid])\n",
    "        kmf.plot()\n",
    "        #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\t#pred_surv=np.mean(np.cumprod(y_pred_valid[F_test==i,:], axis=1),axis=0)\n",
    "\t#plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='--',c='C'+str(i))\n",
    "    else:\n",
    "        kmf.fit(T_test[one_year_survival_prob_valid < one_yr_median_valid ], event_observed=F_test[one_year_survival_prob_valid < one_yr_median_valid])\n",
    "        kmf.plot()\n",
    "        #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\n",
    "N1 = ' N= ' + str(len(T_test[one_year_survival_prob_valid >= one_yr_median_valid ]))\n",
    "N2 = ' N= ' + str(len(T_test[one_year_survival_prob_valid < one_yr_median_valid ]))\n",
    "plt.xticks(np.arange(0, days_plot, 365))\n",
    "plt.yticks(np.arange(0, 1.125, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Probability of survival')\n",
    "plt.legend(['Low Risk Individuals' + N1 ,'High Risk Individuals' + N2 ])\n",
    "#\t'1: Actual','1: Predicted')\n",
    "# \t'2: Actual','2: Predicted',\n",
    "# \t'3: Actual','3: Predicted',\n",
    "# \t'4: Actual','4: Predicted'])\n",
    "plt.title('Test set Kaplan-Meier Curves', fontweight = 'bold')\n",
    "plt.grid()\n",
    "#plt.show()\n",
    "plt.savefig('dummy.png', format = 'png', dpi = 1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full DATASET results\n",
    "y_pred_overall=cox.predict([dataset_mrna,dataset_meth],verbose=0, batch_size = batch_size)\n",
    "\n",
    "one_year_survival_prob_all = np.cumprod(y_pred_overall[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "one_yr_median_all = np.median(one_year_survival_prob_all)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "#plt.subplot(1, 2, 1)\n",
    "days_plot = 365*9\n",
    "for i in range(2):\n",
    "    if i==0:\n",
    "        kmf.fit(t[one_year_survival_prob_all >= one_yr_median_all ], event_observed=f[one_year_survival_prob_all >= one_yr_median_all])\n",
    "        kmf.plot()\n",
    "        #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\t#pred_surv=np.mean(np.cumprod(y_pred_valid[F_test==i,:], axis=1),axis=0)\n",
    "\t#plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='--',c='C'+str(i))\n",
    "    else:\n",
    "        kmf.fit(t[one_year_survival_prob_all < one_yr_median_all ], event_observed=f[one_year_survival_prob_all < one_yr_median_all])\n",
    "        kmf.plot()\n",
    "        #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\n",
    "N1 = ' N= ' + str(len(T_train[one_year_survival_prob_train >= one_yr_median_train ]))\n",
    "N2 = ' N= ' + str(len(T_train[one_year_survival_prob_train < one_yr_median_train ]))\n",
    "\n",
    "plt.xticks(np.arange(0, days_plot, 365))\n",
    "plt.yticks(np.arange(0, 1.125, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Probability of survival')\n",
    "plt.legend(['Low Risk Individuals' + N1 ,'High Risk Individuals' + N2 ])\n",
    "#\t'1: Actual','1: Predicted')\n",
    "# \t'2: Actual','2: Predicted',\n",
    "# \t'3: Actual','3: Predicted',\n",
    "# \t'4: Actual','4: Predicted'])\n",
    "plt.title('Train set Kaplan-Meier Curves', fontweight = 'bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_survival_prob_all = np.cumprod(y_pred_overall[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "one_yr_median_all = np.median(one_year_survival_prob_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[one_year_survival_prob_train >= one_yr_median_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "#plt.subplot(1, 2, 1)\n",
    "days_plot = 365*9\n",
    "for i in range(2):\n",
    "    if i==0:\n",
    "        kmf.fit(t[one_year_survival_prob_all >= one_yr_median_all ], event_observed=f[one_year_survival_prob_all >= one_yr_median_all])\n",
    "        kmf.plot()\n",
    "        #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\t#pred_surv=np.mean(np.cumprod(y_pred_valid[F_test==i,:], axis=1),axis=0)\n",
    "\t#plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='--',c='C'+str(i))\n",
    "    else:\n",
    "        kmf.fit(t[one_year_survival_prob_all < one_yr_median_all ], event_observed=f[one_year_survival_prob_all < one_yr_median_all])\n",
    "        kmf.plot()\n",
    "        #plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\n",
    "N1 = ' N= ' + str(len(T_train[one_year_survival_prob_train >= one_yr_median_train ]))\n",
    "N2 = ' N= ' + str(len(T_train[one_year_survival_prob_train < one_yr_median_train ]))\n",
    "\n",
    "plt.xticks(np.arange(0, days_plot, 365))\n",
    "plt.yticks(np.arange(0, 1.125, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Probability of survival')\n",
    "plt.legend(['Low Risk Individuals' + N1 ,'High Risk Individuals' + N2 ])\n",
    "#\t'1: Actual','1: Predicted')\n",
    "# \t'2: Actual','2: Predicted',\n",
    "# \t'3: Actual','3: Predicted',\n",
    "# \t'4: Actual','4: Predicted'])\n",
    "plt.title('Train set Kaplan-Meier Curves', fontweight = 'bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.savefig('Kaplan-meier.png', dpi = 1200, format = 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_year_survival_prob_train = np.cumprod(y_pred[:,0:np.nonzero(breaks>1875)[0][0]], axis=1)[:,-1]\n",
    "five_year_survival_prob_valid = np.cumprod(y_pred_valid[:,0:np.nonzero(breaks>1875)[0][0]], axis=1)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_yr_median_train = np.median(five_year_survival_prob_train)\n",
    "five_yr_median_valid = np.median(five_year_survival_prob_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "days_plot = 365*9\n",
    "for i in range(2):\n",
    "    if i==0:\n",
    "        kmf.fit(T_train[five_year_survival_prob_train>=five_yr_median_train ],event_observed=F_train[five_year_survival_prob_train >=five_yr_median_train])\n",
    "        #kmf.plot()\n",
    "        plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\t#pred_surv=np.mean(np.cumprod(y_pred_valid[F_test==i,:], axis=1),axis=0)\n",
    "\t#plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='--',c='C'+str(i))\n",
    "    else:\n",
    "        kmf.fit(T_train[five_year_survival_prob_train <five_yr_median_train ],event_observed=F_train[five_year_survival_prob_train<five_yr_median_train])\n",
    "        #kmf.plot()\n",
    "        plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\n",
    "plt.xticks(np.arange(0, days_plot, 365))\n",
    "plt.yticks(np.arange(0, 1.001, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Probability of survival')\n",
    "plt.legend(['Low Risk Individuals','High Risk Individuals'])\n",
    "#\t'1: Actual','1: Predicted')\n",
    "# \t'2: Actual','2: Predicted',\n",
    "# \t'3: Actual','3: Predicted',\n",
    "# \t'4: Actual','4: Predicted'])\n",
    "plt.title('Train set Kaplan-Meier Curves')\n",
    "#plt.show()\n",
    "plt.subplot(1, 2, 2)\n",
    "days_plot = 365*9\n",
    "for i in range(2):\n",
    "    if i==0:\n",
    "        kmf.fit(T_test[five_year_survival_prob_valid>=five_yr_median_valid ],event_observed=F_test[five_year_survival_prob_valid >=five_yr_median_valid])\n",
    "        #kmf.plot()\n",
    "        plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\t#pred_surv=np.mean(np.cumprod(y_pred_valid[F_test==i,:], axis=1),axis=0)\n",
    "\t#plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='--',c='C'+str(i))\n",
    "    else:\n",
    "        kmf.fit(T_test[five_year_survival_prob_valid <five_yr_median_valid ],event_observed=F_test[five_year_survival_prob_valid<five_yr_median_valid])\n",
    "        #kmf.plot()\n",
    "        plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(i))\n",
    "\n",
    "plt.xticks(np.arange(0, days_plot, 365))\n",
    "plt.yticks(np.arange(0, 1.0001, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Probability of survival')\n",
    "plt.legend(['Low Risk Individuals','High Risk Individuals'])\n",
    "#\t'1: Actual','1: Predicted')\n",
    "# \t'2: Actual','2: Predicted',\n",
    "# \t'3: Actual','3: Predicted',\n",
    "# \t'4: Actual','4: Predicted'])\n",
    "plt.title('Train set Kaplan-Meier Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(kmf.survival_function_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(kmf._median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmf.fit(t[g=='female'], event_observed=f[g=='female'])\n",
    "# kmf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ['male','female']:\n",
    "# \tkmf.fit(t[g==i], event_observed=f[g==i])\n",
    "#     #kmf.plot()\n",
    "# \tplt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='-')\n",
    "# \t#pred_surv=np.mean(np.cumprod(y_pred_valid[F_test==i,:], axis=1),axis=0)\n",
    "# \t#predicted.append(plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='--',c='C'+str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual.append(plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='solid',c='C'+str(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_surv=np.mean(np.cumprod(y_pred[F_train==0,:], axis=1),axis=0)\n",
    "pred_surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate(([1],pred_surv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted.append(plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='-',c='C'+str(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_plot = 365*10\n",
    "for i in range(2):\n",
    "\t#kmf.fit(T_train[F_train==i], event_observed=F_train[F_train==i])\n",
    "\t#actual.append(plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='--',c='C'+str(i)))\n",
    "\tpred_surv=np.mean(np.cumprod(y_pred_valid[F_test==i,:], axis=1),axis=0)\n",
    "\tplt.plot(breaks,np.concatenate(([1],pred_surv)),ls='--',c='C'+str(i))\n",
    "plt.xticks(np.arange(0, days_plot, 365))\n",
    "plt.yticks(np.arange(0, 1.0001, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Probability of survival')\n",
    "# plt.legend(['0: Actual','0: Predicted',\n",
    "# \t'1: Actual','1: Predicted',\n",
    "# \t'2: Actual','2: Predicted',\n",
    "# \t'3: Actual','3: Predicted',\n",
    "# \t'4: Actual','4: Predicted'])\n",
    "plt.title('Test set calibration')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = np.mean(np.cumprod(y_pred_valid[F_test==0,:], axis=1),axis=0)\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1concat = np.concatenate(([1],temp1))\n",
    "temp1concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = np.mean(np.cumprod(y_pred_valid[F_test==1,:], axis=1),axis=0)\n",
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2concat = np.concatenate(([1],temp2))\n",
    "temp2concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp2concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model----------------------------------------------------------------------------------------------- toy model\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "# #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# #model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(4, activation='relu'))\n",
    "# prop_hazards=1\n",
    "# if prop_hazards:\n",
    "# \tmodel.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "# \tmodel.add(nnet_survival.PropHazards(n_intervals))\n",
    "# else:\n",
    "# \tmodel.add(Dense(n_intervals, kernel_initializer='zeros', bias_initializer='zeros'))\n",
    "# \tmodel.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.RMSprop(), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history=model.fit_generator(training_set, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.where(event==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_train = len(survival_times)\n",
    "# R_matrix_train = np.zeros([N_train, N_train], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(N_train):\n",
    "#         for j in range(N_train):\n",
    "#             #print (survival_times[j],survival_times[i])\n",
    "#             R_matrix_train[i,j] = survival_times[j] >= survival_times[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_R = np.asarray(R_matrix_train,dtype=np.float32)\n",
    "# train_ystatus = np.asarray(event,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ystatus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.getsizeof(R_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = lbl_pred.reshape(-1)\n",
    "exp_theta = torch.exp(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = lbl_pred.reshape(-1)\n",
    "# exp_theta = torch.exp(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_intervals = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (122, 122, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "# #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# #model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(4, activation='relu'))\n",
    "# prop_hazards=1\n",
    "# if prop_hazards:\n",
    "# \tmodel.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "# \tmodel.add(nnet_survival.PropHazards(n_intervals))\n",
    "# else:\n",
    "# \tmodel.add(Dense(n_intervals, kernel_initializer='zeros', bias_initializer='zeros'))\n",
    "# \tmodel.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to convert PropHazards model of CoxNN and SALMON converted to Keras; outputs prognostics index-------------- yet to succed\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(4, activation='relu'))\n",
    "# #model.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "# #model.add(nnet_survival.PropHazards(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cox = model.layers[-1].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-1].output_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cox = model.add(CoxRegression(input_cox, n_in=model.layers[-1].output_shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-1].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_out,last_layer_output_dim = model.layers[-1].output,model.layers[-1].output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(train_ystatus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = K.reshape(last_layer_out, K.shape(last_layer_out))\n",
    "theta2 = K.reshape(last_layer_out, K.shape(last_layer_output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CoxRegression(object):\n",
    "#     def __init__(self, input, n_in):\n",
    "#         self.W = theano.shared(value=numpy.zeros((n_in,1),dtype=theano.config.floatX), name='W_cox',borrow=True)\n",
    "#         # b_values = numpy.zeros((1,), dtype=theano.config.floatX)\n",
    "#         # self.b = theano.shared(value=b_values, name='b_cox', borrow=True) #intercept term is unnecessary\n",
    "        \n",
    "#         self.input = input[0] if len(input) == 1 else T.concatenate(input, axis=1)\n",
    "#         #self.input = input\n",
    "\n",
    "#         self.theta = T.dot(self.input, self.W) # + self.b\n",
    "#         self.theta = T.reshape(self.theta, newshape=[T.shape(self.theta)[0]]) #recast theta as vector\n",
    "#         self.exp_theta = T.exp(self.theta)\n",
    "#         self.params = [self.W] #, self.b]\n",
    "\n",
    "#     def negative_log_likelihood(self, R_batch, ystatus_batch):\n",
    "#         return(-T.mean((self.theta - T.log(T.sum(self.exp_theta * R_batch,axis=1))) * ystatus_batch)) #exp_theta * R_batch ~ sum the exp_thetas of the patients with greater time e.g., R(t)\n",
    "#         #e.g., all columns of product will have same value or zero, then do a rowSum\n",
    "    \n",
    "#     def evalNewData(self, test_data):\n",
    "#         return(T.dot(T.concatenate(test_data, axis=1), self.W)) # + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoxRegression(Layer):\n",
    "    def __init__(self, input, n_in):\n",
    "        #self.W = tf.Variable(value=numpy.zeros((n_in,1),dtype=K.float), name='W_cox',borrow=True)\n",
    "        self.W = K.variable(value=np.zeros((n_in,1)), dtype='float32', name='W_cox', constraint=None)\n",
    "        # b_values = numpy.zeros((1,), dtype=theano.config.floatX)\n",
    "        # self.b = theano.shared(value=b_values, name='b_cox', borrow=True) #intercept term is unnecessary\n",
    "        \n",
    "        #self.input = input[0] if input.shape[1] == 1 else K.concatenate(input, axis=1)\n",
    "        #self.input =  K.concatenate(input, axis=1)\n",
    "        self.input = input\n",
    "\n",
    "        self.theta = K.dot(self.input, self.W) # + self.b\n",
    "        self.theta = K.reshape(self.theta, K.shape(self.theta)) #recast theta as vector\n",
    "        self.exp_theta = K.exp(self.theta)\n",
    "        self.params = [self.W] #, self.b]\n",
    "\n",
    "    def negative_log_likelihood(self, R_batch, ystatus_batch):\n",
    "        return(-K.mean((self.theta - K.log(K.sum(self.exp_theta * R_batch,axis=1))) * ystatus_batch)) #exp_theta * R_batch ~ sum the exp_thetas of the patients with greater time e.g., R(t)\n",
    "        #e.g., all columns of product will have same value or zero, then do a rowSum\n",
    "    \n",
    "    def evalNewData(self, test_data):\n",
    "        return(K.dot(K.concatenate(test_data, axis=1), self.W)) # + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.is_keras_tensor(last_layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#@tf.function\n",
    "def negative_log_likelihood(self,last_layer_out, R_batch, ystatus_batch):\n",
    "    self.theta = K.reshape(last_layer_out, K.shape(last_layer_out))\n",
    "    self.exp_theta = K.exp(self.theta)\n",
    "    return(-K.mean((theta - K.log(K.sum(exp_theta * R_batch,axis=1))) * ystatus_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss=negative_log_likelihood(last_layer_out, train_R, train_ystatus), optimizer=optimizers.Adam(), metrics = ['accuracy'])\n",
    "#model.compile(loss=nnet_survival.surv_likelihood_edit(1,train_R, train_ystatus), optimizer=optimizers.Adam(), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=optimizers.Adam(), metrics = ['accuracy'])\n",
    "#early_stopping = EarlyStopping(monitor='loss', patience=50)\n",
    "#history=model.fit(x_train, y_train_array, batch_size=16, epochs=10, verbose=1, callbacks=[early_stopping], validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train, y_train_array, batch_size=1, epochs=10, verbose=1,validation_split=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set results\n",
    "y_pred=model.predict_proba(x_train,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " _ = np.nonzero(breaks>2)[0][0]\n",
    " _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discrimination (C-index)\n",
    "#oneyr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "oneyr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>2)[0][0]], axis=1)[:,-1]\n",
    "print(concordance_index(time,oneyr_surv,event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibration plot\n",
    "days_plot = 365*5\n",
    "plt.figure(figsize=(5,5))\n",
    "#plt.subplot(1, 2, 1)\n",
    "kmf = KaplanMeierFitter()\n",
    "matplotlib.style.use('default')\n",
    "actual = []\n",
    "predicted = []\n",
    "for i in range(num_classes):\n",
    "\tkmf.fit(time[y_train==i], event_observed=event[y_train==i])\n",
    "\tactual.append(plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='--',c='C'+str(i)))\n",
    "\tpred_surv=np.mean(np.cumprod(y_pred[y_train==i,:], axis=1),axis=0)\n",
    "\tpredicted.append(plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='-',c='C'+str(i)))\n",
    "\t#print(i, kmf.median_)\n",
    "\n",
    "plt.xticks(np.arange(0, days_plot+0.0001, 200))\n",
    "plt.yticks(np.arange(0, 1.0001, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "#plt.ylabel('Proportion surviving')\n",
    "plt.ylabel('Probability of survival')\n",
    "plt.legend(['0: Actual','0: Predicted',\n",
    "\t'1: Actual','1: Predicted',\n",
    "\t'2: Actual','2: Predicted',\n",
    "\t'3: Actual','3: Predicted',\n",
    "\t'4: Actual','4: Predicted'])\n",
    "plt.title('Training set calibration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set results\n",
    "y_pred=model.predict_proba(x_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discrimination (C-index)\n",
    "oneyr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "print(concordance_index(timeTest,oneyr_surv,eventTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#discrimination of perfect model that uses actual digit as survival time predictor\n",
    "print(concordance_index(timeTest,-y_test.astype('float'),eventTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibration\n",
    "plt.figure(figsize=(10,5))\n",
    "#plt.subplot(1, 2, 2)\n",
    "kmf = KaplanMeierFitter()\n",
    "matplotlib.style.use('default')\n",
    "actual = []\n",
    "predicted = []\n",
    "for i in range(num_classes):\n",
    "\tkmf.fit(timeTest[y_test==i], event_observed=eventTest[y_test==i])\n",
    "\tactual.append(plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='--',c='C'+str(i)))\n",
    "\tpred_surv=np.mean(np.cumprod(y_pred[y_test==i,:], axis=1),axis=0)\n",
    "\tpredicted.append(plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='-',c='C'+str(i)))\n",
    "\n",
    "plt.xticks(np.arange(0, days_plot+0.0001, 200))\n",
    "plt.yticks(np.arange(0, 1.0001, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Probability of survival')\n",
    "plt.legend(['0: Actual','0: Predicted',\n",
    "\t'1: Actual','1: Predicted',\n",
    "\t'2: Actual','2: Predicted',\n",
    "\t'3: Actual','3: Predicted',\n",
    "\t'4: Actual','4: Predicted'])\n",
    "plt.title('Test set calibration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halflife=365.*0.7\n",
    "breaks=-np.log(1-np.arange(0.0,0.96,0.1))*halflife/np.log(2) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_intervals=len(breaks)-1\n",
    "timegap = breaks[1:] - breaks[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_array=nnet_survival.make_surv_array(time,event,breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_interval_width = 0\n",
    "# if fixed_interval_width:\n",
    "# \tbreaks=np.arange(0.,365.*5,365./8)\n",
    "# \tn_intervals=len(breaks)-1\n",
    "# \ttimegap = breaks[1:] - breaks[:-1]\n",
    "# else:\n",
    "# \thalflife=365.*5\n",
    "# \tbreaks=-np.log(1-np.arange(0.0,0.96,0.05))*halflife/np.log(2) \n",
    "# \tn_intervals=len(breaks)-1\n",
    "# \ttimegap = breaks[1:] - breaks[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import mnist\n",
    "\n",
    "# num_classes = 2\n",
    "# img_rows, img_cols = 28, 28\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "# \tx_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "# \tx_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "# \tinput_shape = (1, img_rows, img_cols)\n",
    "# else:\n",
    "# \tx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "# \tx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "# \tinput_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "# #Only keep numbers 0-4\n",
    "# #Only keep numbers 0-1\n",
    "# x_train = x_train[y_train<2, :, :, :]\n",
    "# x_test = x_test[y_test<2, :, :, :]\n",
    "# y_train = y_train[y_train<2]\n",
    "# y_test = y_test[y_test<2]\n",
    "\n",
    "# #Create simulated survival data, with higher numbers having shorter average survival\n",
    "# sampleSizeTrain = x_train.shape[0]\n",
    "# sampleSizeTest = x_test.shape[0]\n",
    "# np.random.seed(0)\n",
    "# beta = 0.9\n",
    "# lambdaT = 365./np.log(2)\n",
    "# lambdaC = 2*365./np.log(2)\n",
    "# trueTime = np.random.exponential(scale = lambdaT * np.exp(-(beta*y_train)),size=sampleSizeTrain)\n",
    "# #median surv is: 365.*np.exp(-beta*np.array([0,1,2,3,4]))\n",
    "# #[365., 148.39792581,  60.3340942 ,  24.53001215, 9.97315869]\n",
    "# censoringTime = np.random.exponential(scale = lambdaC, size=sampleSizeTrain)\n",
    "# time = np.minimum(trueTime, censoringTime)\n",
    "# event = (time == trueTime)*1.\n",
    "\n",
    "\n",
    "# #for test data-----------------------------------------------------------------------------------------\n",
    "# # trueTimeTest = np.random.exponential(scale = lambdaT * np.exp(-(beta*y_test)),size=sampleSizeTest)\n",
    "# # censoringTimeTest = np.random.exponential(scale = lambdaC, size=sampleSizeTest)\n",
    "# # timeTest = np.minimum(trueTimeTest, censoringTimeTest)\n",
    "# # eventTest = (timeTest == trueTimeTest)*1.\n",
    "\n",
    "# #Convert event data to array format\n",
    "\n",
    "# #halflife=365.*0.7\n",
    "# #breaks=-np.log(1-np.arange(0.0,0.96,0.1))*halflife/np.log(2) \n",
    "# #breaks=np.concatenate((np.arange(0,200,10),np.arange(200,1001,25)))\n",
    "\n",
    "# #n_intervals=len(breaks)-1\n",
    "# #timegap = breaks[1:] - breaks[:-1]\n",
    "# #y_train_array=nnet_survival.make_surv_array(time,event,breaks)\n",
    "\n",
    "# #Train model\n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "# # from tensorflow import set_random_seed\n",
    "# # set_random_seed(1)\n",
    "# import tensorflow\n",
    "# tensorflow.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "#training_list.sort()\n",
    "#len(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "#test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "#train_datagen.fit(X_train)\n",
    "#test_datagen.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}